{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ec326dd-23b2-4356-a8dc-948d2ba53eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from auth import get_token\n",
    "import requests\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7326a3e-23f0-4c32-a22e-b0442ba8edba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_token in module auth:\n",
      "\n",
      "get_token(client_id: str, client_secret: str)\n",
      "    Fetch a token from Pole Emploi API \n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    client_id : str\n",
      "        \n",
      "    client_secret : str\n",
      "      \n",
      "    Returns\n",
      "    -------\n",
      "    str\n",
      "        a token\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(get_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274cc52f-ba7d-4e70-86b7-d16fa21c6b56",
   "metadata": {},
   "source": [
    "**Get token function from auth.py file**\n",
    "1. To find client_id and client_secret, go to pole emploi documentation\n",
    "2. Create a new application, bind the application to a chosen API, validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b982a908-b969-4a42-b65b-e45abdc77d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access Token: qyvuWAVnIlq5Oo0lMKsMrpmwum8\n"
     ]
    }
   ],
   "source": [
    "access_token=get_token( client_id=\"PAR_paulemploi_cc096c4476ee9edb9205316b64b8331dc387b1ac6ade7cbf11dcbae2010fdb69\", client_secret=\"fddb798e0b6f9b333688c6770f3170279c36c8fc17220533528fc760e50b63a8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d401a373-206a-4e70-b040-20632d6c95b0",
   "metadata": {},
   "source": [
    "**Get API content**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5cf497fe-3cbb-47c2-b030-320cd882089d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [206]>\n"
     ]
    }
   ],
   "source": [
    "keyword = 'data'\n",
    "domaine = 'M18'\n",
    "\n",
    "url = 'https://api.pole-emploi.io/partenaire/offresdemploi/v2/offres/search'\n",
    "headers = {\n",
    "    'accept': 'application/json',\n",
    "    'Authorization': f'Bearer {access_token}'#variable access_token\n",
    "}\n",
    "params = {\n",
    "    'domaine':domaine ,'motsCles': keyword ,'pays':'France'#params in pole emploi documentation\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers, params=params)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0af92e17-08e0-4ead-a3d0-d491e2bca80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_json=response.json()#put the response in json type\n",
    "#response_json\n",
    "#we will get list in a dictonnary with two keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0fb87872-7174-4fbc-983b-fd4b5bb9f69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['resultats', 'filtresPossibles'])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_json.keys() #dict keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48c34bcc-98ce-4bba-8ddb-e048fc392d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'filtre': 'typeContrat',\n",
       "  'agregation': [{'valeurPossible': 'CDD', 'nbResultats': 22},\n",
       "   {'valeurPossible': 'CDI', 'nbResultats': 316},\n",
       "   {'valeurPossible': 'FRA', 'nbResultats': 5},\n",
       "   {'valeurPossible': 'LIB', 'nbResultats': 113},\n",
       "   {'valeurPossible': 'MIS', 'nbResultats': 7}]},\n",
       " {'filtre': 'experience',\n",
       "  'agregation': [{'valeurPossible': '0', 'nbResultats': 60},\n",
       "   {'valeurPossible': '1', 'nbResultats': 194},\n",
       "   {'valeurPossible': '2', 'nbResultats': 152},\n",
       "   {'valeurPossible': '3', 'nbResultats': 57}]},\n",
       " {'filtre': 'qualification',\n",
       "  'agregation': [{'valeurPossible': '0', 'nbResultats': 19},\n",
       "   {'valeurPossible': '9', 'nbResultats': 144},\n",
       "   {'valeurPossible': 'X', 'nbResultats': 300}]},\n",
       " {'filtre': 'natureContrat',\n",
       "  'agregation': [{'valeurPossible': 'E1', 'nbResultats': 341},\n",
       "   {'valeurPossible': 'E2', 'nbResultats': 4},\n",
       "   {'valeurPossible': 'NS', 'nbResultats': 118}]}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_json['filtresPossibles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90b49e3e-dc45-4f08-b532-bb56531dc4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultat=response_json['resultats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e240b15-1889-4192-bf90-29162c1afe91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(resultat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3f9b370-cd53-42a2-9446-920ecbc26c0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>intitule</th>\n",
       "      <th>description</th>\n",
       "      <th>dateCreation</th>\n",
       "      <th>dateActualisation</th>\n",
       "      <th>lieuTravail</th>\n",
       "      <th>romeCode</th>\n",
       "      <th>romeLibelle</th>\n",
       "      <th>appellationlibelle</th>\n",
       "      <th>entreprise</th>\n",
       "      <th>...</th>\n",
       "      <th>qualitesProfessionnelles</th>\n",
       "      <th>origineOffre</th>\n",
       "      <th>offresManqueCandidats</th>\n",
       "      <th>deplacementCode</th>\n",
       "      <th>deplacementLibelle</th>\n",
       "      <th>formations</th>\n",
       "      <th>langues</th>\n",
       "      <th>agence</th>\n",
       "      <th>experienceCommentaire</th>\n",
       "      <th>complementExercice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>166XMTR</td>\n",
       "      <td>Technicien backup Data Center (H/F)</td>\n",
       "      <td>METALINE, ESN spécialisée dans le recrutement ...</td>\n",
       "      <td>2023-12-29T09:54:37.000Z</td>\n",
       "      <td>2023-12-29T14:34:27.000Z</td>\n",
       "      <td>{'libelle': '28 - CHARTRES', 'latitude': 48.44...</td>\n",
       "      <td>M1810</td>\n",
       "      <td>Production et exploitation de systèmes d'infor...</td>\n",
       "      <td>Adjoint(e) technicien(ne) d'exploitation infor...</td>\n",
       "      <td>{'nom': 'METALINE', 'description': 'METALOGIC ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'libelle': 'Travailler en équipe', 'descript...</td>\n",
       "      <td>{'origine': '1', 'urlOrigine': 'https://candid...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>166XFCD</td>\n",
       "      <td>TALENTUEUX CONSULTANT BI / BIG DATA - LILLE - ...</td>\n",
       "      <td>TA MISSION :\\nRattaché à l'une de nos agences,...</td>\n",
       "      <td>2023-12-28T16:50:04.000Z</td>\n",
       "      <td>2023-12-29T13:54:20.000Z</td>\n",
       "      <td>{'libelle': '59 - LILLE', 'latitude': 50.63099...</td>\n",
       "      <td>M1805</td>\n",
       "      <td>Études et développement informatique</td>\n",
       "      <td>Analyste décisionnel - Business Intelligence</td>\n",
       "      <td>{'nom': 'NEXT DECISION', 'description': 'Fondé...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'libelle': 'S'adapter aux changements', 'des...</td>\n",
       "      <td>{'origine': '1', 'urlOrigine': 'https://candid...</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>Ponctuels Zone départementale</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>166VKBR</td>\n",
       "      <td>Consultant Oracle Data Integration (H/F)</td>\n",
       "      <td>Dans le cadre de développement de ses activité...</td>\n",
       "      <td>2023-12-27T09:40:34.000Z</td>\n",
       "      <td>2023-12-28T02:32:44.000Z</td>\n",
       "      <td>{'libelle': '92 - COURBEVOIE', 'latitude': 48....</td>\n",
       "      <td>M1806</td>\n",
       "      <td>Conseil et maîtrise d'ouvrage en systèmes d'in...</td>\n",
       "      <td>Consultant / Consultante en système d'information</td>\n",
       "      <td>{'nom': 'KWET CONSEIL', 'description': 'Sociét...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'libelle': 'Travailler en équipe', 'descript...</td>\n",
       "      <td>{'origine': '1', 'urlOrigine': 'https://candid...</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>Quotidiens Zone nationale</td>\n",
       "      <td>[{'niveauLibelle': 'Bac+5 et plus ou équivalen...</td>\n",
       "      <td>[{'libelle': 'Anglais', 'exigence': 'E'}]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>166SLVL</td>\n",
       "      <td>Product Owner Data (H/F)</td>\n",
       "      <td>Dans le cadre de notre développement, nous rec...</td>\n",
       "      <td>2023-12-22T11:56:51.000Z</td>\n",
       "      <td>2023-12-29T09:36:47.000Z</td>\n",
       "      <td>{'libelle': '69 - LYON 06', 'latitude': 45.772...</td>\n",
       "      <td>M1806</td>\n",
       "      <td>Conseil et maîtrise d'ouvrage en systèmes d'in...</td>\n",
       "      <td>Product Owner</td>\n",
       "      <td>{'nom': 'PROXIAD', 'description': 'Créée en 19...</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'libelle': 'Travailler en équipe', 'descript...</td>\n",
       "      <td>{'origine': '1', 'urlOrigine': 'https://candid...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>Jamais</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166RQMR</td>\n",
       "      <td>Data Engineer (H/F)</td>\n",
       "      <td>Si vous pensez que la Data et l'Intelligence A...</td>\n",
       "      <td>2023-12-21T17:10:08.000Z</td>\n",
       "      <td>2023-12-28T23:58:35.000Z</td>\n",
       "      <td>{'libelle': '75 - PARIS 09', 'latitude': 48.87...</td>\n",
       "      <td>M1802</td>\n",
       "      <td>Expertise et support en systèmes d'information</td>\n",
       "      <td>Data manager</td>\n",
       "      <td>{'entrepriseAdaptee': False}</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'libelle': 'Prendre des initiatives et être ...</td>\n",
       "      <td>{'origine': '1', 'urlOrigine': 'https://candid...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>8394828</td>\n",
       "      <td>RESPONSABLE COMPTABLE / CHIEF DATA OFFICER F/H...</td>\n",
       "      <td>Descriptif du poste:\\n\\n                \\nEN B...</td>\n",
       "      <td>2023-12-21T09:45:12.000Z</td>\n",
       "      <td>2023-12-21T09:45:12.000Z</td>\n",
       "      <td>{'libelle': '33 - CENON', 'latitude': 44.85493...</td>\n",
       "      <td>M1802</td>\n",
       "      <td>Expertise et support en systèmes d'information</td>\n",
       "      <td>Data manager</td>\n",
       "      <td>{'nom': 'Adsearch', 'description': 'Adsearch v...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'origine': '2', 'urlOrigine': 'https://candid...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>8394425</td>\n",
       "      <td>Data Engineer H/F</td>\n",
       "      <td>Vous rejoignez l'équipe IT dans laquelle vous ...</td>\n",
       "      <td>2023-12-21T09:43:14.000Z</td>\n",
       "      <td>2023-12-21T09:55:16.000Z</td>\n",
       "      <td>{'libelle': '73 - CHAMBERY', 'latitude': 45.58...</td>\n",
       "      <td>M1802</td>\n",
       "      <td>Expertise et support en systèmes d'information</td>\n",
       "      <td>Data manager</td>\n",
       "      <td>{'nom': 'Hays France', 'description': 'Notre c...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'origine': '2', 'urlOrigine': 'https://candid...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>8336731</td>\n",
       "      <td>RESPONSABLE DU DOMAINE DATA &amp; BI F/H - Exploit...</td>\n",
       "      <td>Descriptif du poste:\\n\\n \\n\\nEn tant que Respo...</td>\n",
       "      <td>2023-12-20T08:57:53.000Z</td>\n",
       "      <td>2023-12-21T09:41:55.000Z</td>\n",
       "      <td>{'libelle': '972 - LE LAMENTIN', 'latitude': 1...</td>\n",
       "      <td>M1803</td>\n",
       "      <td>Direction des systèmes d'information</td>\n",
       "      <td>Responsable de domaine en informatique</td>\n",
       "      <td>{'nom': 'INFOBAM', 'description': ' \n",
       "\n",
       "GBH empl...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'origine': '2', 'urlOrigine': 'https://candid...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>8335556</td>\n",
       "      <td>Data engineer F/H - Système, réseaux, données ...</td>\n",
       "      <td>Descriptif du poste:\\n\\nLe profil sélectionné ...</td>\n",
       "      <td>2023-12-20T08:50:55.000Z</td>\n",
       "      <td>2023-12-20T08:50:55.000Z</td>\n",
       "      <td>{'libelle': '75 - PARIS 01', 'latitude': 48.85...</td>\n",
       "      <td>M1801</td>\n",
       "      <td>Administration de systèmes d'information</td>\n",
       "      <td>Administrateur / Administratrice de plateforme...</td>\n",
       "      <td>{'nom': 'INGENIANCE', 'description': 'Depuis 2...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'origine': '2', 'urlOrigine': 'https://candid...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>8334817</td>\n",
       "      <td>ALTERNANCE - DATA MANAGEMENT - SEPTEMBRE 2024 ...</td>\n",
       "      <td>Descriptif du poste:\\n\\nRattaché au Manager DA...</td>\n",
       "      <td>2023-12-20T08:46:40.000Z</td>\n",
       "      <td>2023-12-20T08:46:40.000Z</td>\n",
       "      <td>{'libelle': '85 - POUZAUGES', 'latitude': 46.7...</td>\n",
       "      <td>M1803</td>\n",
       "      <td>Direction des systèmes d'information</td>\n",
       "      <td>Responsable informatique</td>\n",
       "      <td>{'nom': 'FLEURY MICHON', 'description': 'Pourq...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'origine': '2', 'urlOrigine': 'https://candid...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                           intitule  \\\n",
       "0    166XMTR                Technicien backup Data Center (H/F)   \n",
       "1    166XFCD  TALENTUEUX CONSULTANT BI / BIG DATA - LILLE - ...   \n",
       "2    166VKBR           Consultant Oracle Data Integration (H/F)   \n",
       "3    166SLVL                           Product Owner Data (H/F)   \n",
       "4    166RQMR                                Data Engineer (H/F)   \n",
       "..       ...                                                ...   \n",
       "145  8394828  RESPONSABLE COMPTABLE / CHIEF DATA OFFICER F/H...   \n",
       "146  8394425                                  Data Engineer H/F   \n",
       "147  8336731  RESPONSABLE DU DOMAINE DATA & BI F/H - Exploit...   \n",
       "148  8335556  Data engineer F/H - Système, réseaux, données ...   \n",
       "149  8334817  ALTERNANCE - DATA MANAGEMENT - SEPTEMBRE 2024 ...   \n",
       "\n",
       "                                           description  \\\n",
       "0    METALINE, ESN spécialisée dans le recrutement ...   \n",
       "1    TA MISSION :\\nRattaché à l'une de nos agences,...   \n",
       "2    Dans le cadre de développement de ses activité...   \n",
       "3    Dans le cadre de notre développement, nous rec...   \n",
       "4    Si vous pensez que la Data et l'Intelligence A...   \n",
       "..                                                 ...   \n",
       "145  Descriptif du poste:\\n\\n                \\nEN B...   \n",
       "146  Vous rejoignez l'équipe IT dans laquelle vous ...   \n",
       "147  Descriptif du poste:\\n\\n \\n\\nEn tant que Respo...   \n",
       "148  Descriptif du poste:\\n\\nLe profil sélectionné ...   \n",
       "149  Descriptif du poste:\\n\\nRattaché au Manager DA...   \n",
       "\n",
       "                 dateCreation         dateActualisation  \\\n",
       "0    2023-12-29T09:54:37.000Z  2023-12-29T14:34:27.000Z   \n",
       "1    2023-12-28T16:50:04.000Z  2023-12-29T13:54:20.000Z   \n",
       "2    2023-12-27T09:40:34.000Z  2023-12-28T02:32:44.000Z   \n",
       "3    2023-12-22T11:56:51.000Z  2023-12-29T09:36:47.000Z   \n",
       "4    2023-12-21T17:10:08.000Z  2023-12-28T23:58:35.000Z   \n",
       "..                        ...                       ...   \n",
       "145  2023-12-21T09:45:12.000Z  2023-12-21T09:45:12.000Z   \n",
       "146  2023-12-21T09:43:14.000Z  2023-12-21T09:55:16.000Z   \n",
       "147  2023-12-20T08:57:53.000Z  2023-12-21T09:41:55.000Z   \n",
       "148  2023-12-20T08:50:55.000Z  2023-12-20T08:50:55.000Z   \n",
       "149  2023-12-20T08:46:40.000Z  2023-12-20T08:46:40.000Z   \n",
       "\n",
       "                                           lieuTravail romeCode  \\\n",
       "0    {'libelle': '28 - CHARTRES', 'latitude': 48.44...    M1810   \n",
       "1    {'libelle': '59 - LILLE', 'latitude': 50.63099...    M1805   \n",
       "2    {'libelle': '92 - COURBEVOIE', 'latitude': 48....    M1806   \n",
       "3    {'libelle': '69 - LYON 06', 'latitude': 45.772...    M1806   \n",
       "4    {'libelle': '75 - PARIS 09', 'latitude': 48.87...    M1802   \n",
       "..                                                 ...      ...   \n",
       "145  {'libelle': '33 - CENON', 'latitude': 44.85493...    M1802   \n",
       "146  {'libelle': '73 - CHAMBERY', 'latitude': 45.58...    M1802   \n",
       "147  {'libelle': '972 - LE LAMENTIN', 'latitude': 1...    M1803   \n",
       "148  {'libelle': '75 - PARIS 01', 'latitude': 48.85...    M1801   \n",
       "149  {'libelle': '85 - POUZAUGES', 'latitude': 46.7...    M1803   \n",
       "\n",
       "                                           romeLibelle  \\\n",
       "0    Production et exploitation de systèmes d'infor...   \n",
       "1                 Études et développement informatique   \n",
       "2    Conseil et maîtrise d'ouvrage en systèmes d'in...   \n",
       "3    Conseil et maîtrise d'ouvrage en systèmes d'in...   \n",
       "4       Expertise et support en systèmes d'information   \n",
       "..                                                 ...   \n",
       "145     Expertise et support en systèmes d'information   \n",
       "146     Expertise et support en systèmes d'information   \n",
       "147               Direction des systèmes d'information   \n",
       "148           Administration de systèmes d'information   \n",
       "149               Direction des systèmes d'information   \n",
       "\n",
       "                                    appellationlibelle  \\\n",
       "0    Adjoint(e) technicien(ne) d'exploitation infor...   \n",
       "1         Analyste décisionnel - Business Intelligence   \n",
       "2    Consultant / Consultante en système d'information   \n",
       "3                                        Product Owner   \n",
       "4                                         Data manager   \n",
       "..                                                 ...   \n",
       "145                                       Data manager   \n",
       "146                                       Data manager   \n",
       "147             Responsable de domaine en informatique   \n",
       "148  Administrateur / Administratrice de plateforme...   \n",
       "149                           Responsable informatique   \n",
       "\n",
       "                                            entreprise  ...  \\\n",
       "0    {'nom': 'METALINE', 'description': 'METALOGIC ...  ...   \n",
       "1    {'nom': 'NEXT DECISION', 'description': 'Fondé...  ...   \n",
       "2    {'nom': 'KWET CONSEIL', 'description': 'Sociét...  ...   \n",
       "3    {'nom': 'PROXIAD', 'description': 'Créée en 19...  ...   \n",
       "4                         {'entrepriseAdaptee': False}  ...   \n",
       "..                                                 ...  ...   \n",
       "145  {'nom': 'Adsearch', 'description': 'Adsearch v...  ...   \n",
       "146  {'nom': 'Hays France', 'description': 'Notre c...  ...   \n",
       "147  {'nom': 'INFOBAM', 'description': ' \n",
       "\n",
       "GBH empl...  ...   \n",
       "148  {'nom': 'INGENIANCE', 'description': 'Depuis 2...  ...   \n",
       "149  {'nom': 'FLEURY MICHON', 'description': 'Pourq...  ...   \n",
       "\n",
       "                              qualitesProfessionnelles  \\\n",
       "0    [{'libelle': 'Travailler en équipe', 'descript...   \n",
       "1    [{'libelle': 'S'adapter aux changements', 'des...   \n",
       "2    [{'libelle': 'Travailler en équipe', 'descript...   \n",
       "3    [{'libelle': 'Travailler en équipe', 'descript...   \n",
       "4    [{'libelle': 'Prendre des initiatives et être ...   \n",
       "..                                                 ...   \n",
       "145                                                NaN   \n",
       "146                                                NaN   \n",
       "147                                                NaN   \n",
       "148                                                NaN   \n",
       "149                                                NaN   \n",
       "\n",
       "                                          origineOffre offresManqueCandidats  \\\n",
       "0    {'origine': '1', 'urlOrigine': 'https://candid...                 False   \n",
       "1    {'origine': '1', 'urlOrigine': 'https://candid...                 False   \n",
       "2    {'origine': '1', 'urlOrigine': 'https://candid...                 False   \n",
       "3    {'origine': '1', 'urlOrigine': 'https://candid...                 False   \n",
       "4    {'origine': '1', 'urlOrigine': 'https://candid...                 False   \n",
       "..                                                 ...                   ...   \n",
       "145  {'origine': '2', 'urlOrigine': 'https://candid...                   NaN   \n",
       "146  {'origine': '2', 'urlOrigine': 'https://candid...                   NaN   \n",
       "147  {'origine': '2', 'urlOrigine': 'https://candid...                   NaN   \n",
       "148  {'origine': '2', 'urlOrigine': 'https://candid...                   NaN   \n",
       "149  {'origine': '2', 'urlOrigine': 'https://candid...                   NaN   \n",
       "\n",
       "    deplacementCode             deplacementLibelle  \\\n",
       "0               NaN                            NaN   \n",
       "1                 2  Ponctuels Zone départementale   \n",
       "2                 4      Quotidiens Zone nationale   \n",
       "3                 1                         Jamais   \n",
       "4               NaN                            NaN   \n",
       "..              ...                            ...   \n",
       "145             NaN                            NaN   \n",
       "146             NaN                            NaN   \n",
       "147             NaN                            NaN   \n",
       "148             NaN                            NaN   \n",
       "149             NaN                            NaN   \n",
       "\n",
       "                                            formations  \\\n",
       "0                                                  NaN   \n",
       "1                                                  NaN   \n",
       "2    [{'niveauLibelle': 'Bac+5 et plus ou équivalen...   \n",
       "3                                                  NaN   \n",
       "4                                                  NaN   \n",
       "..                                                 ...   \n",
       "145                                                NaN   \n",
       "146                                                NaN   \n",
       "147                                                NaN   \n",
       "148                                                NaN   \n",
       "149                                                NaN   \n",
       "\n",
       "                                       langues agence experienceCommentaire  \\\n",
       "0                                          NaN    NaN                   NaN   \n",
       "1                                          NaN    NaN                   NaN   \n",
       "2    [{'libelle': 'Anglais', 'exigence': 'E'}]    NaN                   NaN   \n",
       "3                                          NaN    NaN                   NaN   \n",
       "4                                          NaN    NaN                   NaN   \n",
       "..                                         ...    ...                   ...   \n",
       "145                                        NaN    NaN                   NaN   \n",
       "146                                        NaN    NaN                   NaN   \n",
       "147                                        NaN    NaN                   NaN   \n",
       "148                                        NaN    NaN                   NaN   \n",
       "149                                        NaN    NaN                   NaN   \n",
       "\n",
       "    complementExercice  \n",
       "0                  NaN  \n",
       "1                  NaN  \n",
       "2                  NaN  \n",
       "3                  NaN  \n",
       "4                  NaN  \n",
       "..                 ...  \n",
       "145                NaN  \n",
       "146                NaN  \n",
       "147                NaN  \n",
       "148                NaN  \n",
       "149                NaN  \n",
       "\n",
       "[150 rows x 39 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resultat=pd.DataFrame(resultat)\n",
    "df_resultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bf23b089-f2e9-466d-b11a-b00ce4af46dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'intitule', 'description', 'dateCreation', 'dateActualisation',\n",
       "       'lieuTravail', 'romeCode', 'romeLibelle', 'appellationlibelle',\n",
       "       'entreprise', 'typeContrat', 'typeContratLibelle', 'natureContrat',\n",
       "       'experienceExige', 'experienceLibelle', 'permis', 'competences',\n",
       "       'salaire', 'dureeTravailLibelle', 'dureeTravailLibelleConverti',\n",
       "       'alternance', 'contact', 'nombrePostes', 'accessibleTH',\n",
       "       'qualificationCode', 'qualificationLibelle', 'codeNAF',\n",
       "       'secteurActivite', 'secteurActiviteLibelle', 'qualitesProfessionnelles',\n",
       "       'origineOffre', 'offresManqueCandidats', 'deplacementCode',\n",
       "       'deplacementLibelle', 'formations', 'langues', 'agence',\n",
       "       'experienceCommentaire', 'complementExercice'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resultat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4176f22b-8bcb-4c27-bff3-b05deab78aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "colonnes_a_garder= ['id', 'intitule', 'description', 'dateCreation', 'lieuTravail', 'appellationlibelle', 'entreprise', 'typeContrat','experienceLibelle', 'competences','salaire','alternance', 'secteurActiviteLibelle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aea5ea19-2dfb-48ac-89e4-11c3e1a785c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>intitule</th>\n",
       "      <th>description</th>\n",
       "      <th>dateCreation</th>\n",
       "      <th>lieuTravail</th>\n",
       "      <th>appellationlibelle</th>\n",
       "      <th>entreprise</th>\n",
       "      <th>typeContrat</th>\n",
       "      <th>experienceLibelle</th>\n",
       "      <th>competences</th>\n",
       "      <th>salaire</th>\n",
       "      <th>alternance</th>\n",
       "      <th>secteurActiviteLibelle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>166XMTR</td>\n",
       "      <td>Technicien backup Data Center (H/F)</td>\n",
       "      <td>METALINE, ESN spécialisée dans le recrutement ...</td>\n",
       "      <td>2023-12-29T09:54:37.000Z</td>\n",
       "      <td>{'libelle': '28 - CHARTRES', 'latitude': 48.44...</td>\n",
       "      <td>Adjoint(e) technicien(ne) d'exploitation infor...</td>\n",
       "      <td>{'nom': 'METALINE', 'description': 'METALOGIC ...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>6 mois</td>\n",
       "      <td>[{'code': '102175', 'libelle': 'Réaliser la ma...</td>\n",
       "      <td>{'commentaire': 'Selon profils', 'complement1'...</td>\n",
       "      <td>False</td>\n",
       "      <td>Conseil en systèmes et logiciels informatiques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>166XFCD</td>\n",
       "      <td>TALENTUEUX CONSULTANT BI / BIG DATA - LILLE - ...</td>\n",
       "      <td>TA MISSION :\\nRattaché à l'une de nos agences,...</td>\n",
       "      <td>2023-12-28T16:50:04.000Z</td>\n",
       "      <td>{'libelle': '59 - LILLE', 'latitude': 50.63099...</td>\n",
       "      <td>Analyste décisionnel - Business Intelligence</td>\n",
       "      <td>{'nom': 'NEXT DECISION', 'description': 'Fondé...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>3 ans</td>\n",
       "      <td>[{'code': '109882', 'libelle': 'SQL', 'exigenc...</td>\n",
       "      <td>{'libelle': 'Annuel de 32000,00 Euros à 45000,...</td>\n",
       "      <td>False</td>\n",
       "      <td>Conseil en systèmes et logiciels informatiques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>166VKBR</td>\n",
       "      <td>Consultant Oracle Data Integration (H/F)</td>\n",
       "      <td>Dans le cadre de développement de ses activité...</td>\n",
       "      <td>2023-12-27T09:40:34.000Z</td>\n",
       "      <td>{'libelle': '92 - COURBEVOIE', 'latitude': 48....</td>\n",
       "      <td>Consultant / Consultante en système d'information</td>\n",
       "      <td>{'nom': 'KWET CONSEIL', 'description': 'Sociét...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>2 ans</td>\n",
       "      <td>[{'code': '109818', 'libelle': 'Élaborer des s...</td>\n",
       "      <td>{'libelle': 'Annuel de 35000,00 Euros à 38000,...</td>\n",
       "      <td>False</td>\n",
       "      <td>Conseil pour les affaires et autres conseils d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>166SLVL</td>\n",
       "      <td>Product Owner Data (H/F)</td>\n",
       "      <td>Dans le cadre de notre développement, nous rec...</td>\n",
       "      <td>2023-12-22T11:56:51.000Z</td>\n",
       "      <td>{'libelle': '69 - LYON 06', 'latitude': 45.772...</td>\n",
       "      <td>Product Owner</td>\n",
       "      <td>{'nom': 'PROXIAD', 'description': 'Créée en 19...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>3 ans</td>\n",
       "      <td>[{'code': '109818', 'libelle': 'Élaborer des s...</td>\n",
       "      <td>{'libelle': 'Annuel de 25000,00 Euros à 45000,...</td>\n",
       "      <td>False</td>\n",
       "      <td>Conseil en systèmes et logiciels informatiques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166RQMR</td>\n",
       "      <td>Data Engineer (H/F)</td>\n",
       "      <td>Si vous pensez que la Data et l'Intelligence A...</td>\n",
       "      <td>2023-12-21T17:10:08.000Z</td>\n",
       "      <td>{'libelle': '75 - PARIS 09', 'latitude': 48.87...</td>\n",
       "      <td>Data manager</td>\n",
       "      <td>{'entrepriseAdaptee': False}</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Débutant accepté</td>\n",
       "      <td>[{'code': '300195', 'libelle': 'Concevoir un l...</td>\n",
       "      <td>{'libelle': 'Annuel de 45000,00 Euros à 55000,...</td>\n",
       "      <td>False</td>\n",
       "      <td>Ingénierie, études techniques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>8394828</td>\n",
       "      <td>RESPONSABLE COMPTABLE / CHIEF DATA OFFICER F/H...</td>\n",
       "      <td>Descriptif du poste:\\n\\n                \\nEN B...</td>\n",
       "      <td>2023-12-21T09:45:12.000Z</td>\n",
       "      <td>{'libelle': '33 - CENON', 'latitude': 44.85493...</td>\n",
       "      <td>Data manager</td>\n",
       "      <td>{'nom': 'Adsearch', 'description': 'Adsearch v...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Expérience exigée de 2 An(s)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'commentaire': '40 - 50 k€ brut annuel'}</td>\n",
       "      <td>False</td>\n",
       "      <td>Activités des agences de placement de main-d'œ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>8394425</td>\n",
       "      <td>Data Engineer H/F</td>\n",
       "      <td>Vous rejoignez l'équipe IT dans laquelle vous ...</td>\n",
       "      <td>2023-12-21T09:43:14.000Z</td>\n",
       "      <td>{'libelle': '73 - CHAMBERY', 'latitude': 45.58...</td>\n",
       "      <td>Data manager</td>\n",
       "      <td>{'nom': 'Hays France', 'description': 'Notre c...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Expérience exigée de 5 An(s)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'libelle': 'Annuel de 45000,00 Euros à 55000,...</td>\n",
       "      <td>False</td>\n",
       "      <td>Autres transports routiers de voyageurs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>8336731</td>\n",
       "      <td>RESPONSABLE DU DOMAINE DATA &amp; BI F/H - Exploit...</td>\n",
       "      <td>Descriptif du poste:\\n\\n \\n\\nEn tant que Respo...</td>\n",
       "      <td>2023-12-20T08:57:53.000Z</td>\n",
       "      <td>{'libelle': '972 - LE LAMENTIN', 'latitude': 1...</td>\n",
       "      <td>Responsable de domaine en informatique</td>\n",
       "      <td>{'nom': 'INFOBAM', 'description': ' \n",
       "\n",
       "GBH empl...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Expérience exigée de 5 An(s)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'commentaire': 'A négocier'}</td>\n",
       "      <td>False</td>\n",
       "      <td>Autres activités informatiques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>8335556</td>\n",
       "      <td>Data engineer F/H - Système, réseaux, données ...</td>\n",
       "      <td>Descriptif du poste:\\n\\nLe profil sélectionné ...</td>\n",
       "      <td>2023-12-20T08:50:55.000Z</td>\n",
       "      <td>{'libelle': '75 - PARIS 01', 'latitude': 48.85...</td>\n",
       "      <td>Administrateur / Administratrice de plateforme...</td>\n",
       "      <td>{'nom': 'INGENIANCE', 'description': 'Depuis 2...</td>\n",
       "      <td>CDI</td>\n",
       "      <td>Expérience exigée de 1 An(s)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'commentaire': '40 - 44 k€ brut annuel'}</td>\n",
       "      <td>False</td>\n",
       "      <td>Conseil en systèmes et logiciels informatiques</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>8334817</td>\n",
       "      <td>ALTERNANCE - DATA MANAGEMENT - SEPTEMBRE 2024 ...</td>\n",
       "      <td>Descriptif du poste:\\n\\nRattaché au Manager DA...</td>\n",
       "      <td>2023-12-20T08:46:40.000Z</td>\n",
       "      <td>{'libelle': '85 - POUZAUGES', 'latitude': 46.7...</td>\n",
       "      <td>Responsable informatique</td>\n",
       "      <td>{'nom': 'FLEURY MICHON', 'description': 'Pourq...</td>\n",
       "      <td>CDD</td>\n",
       "      <td>Débutant accepté</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'commentaire': '22 - 25 k€ brut annuel'}</td>\n",
       "      <td>True</td>\n",
       "      <td>Fabrication de plats préparés</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                           intitule  \\\n",
       "0    166XMTR                Technicien backup Data Center (H/F)   \n",
       "1    166XFCD  TALENTUEUX CONSULTANT BI / BIG DATA - LILLE - ...   \n",
       "2    166VKBR           Consultant Oracle Data Integration (H/F)   \n",
       "3    166SLVL                           Product Owner Data (H/F)   \n",
       "4    166RQMR                                Data Engineer (H/F)   \n",
       "..       ...                                                ...   \n",
       "145  8394828  RESPONSABLE COMPTABLE / CHIEF DATA OFFICER F/H...   \n",
       "146  8394425                                  Data Engineer H/F   \n",
       "147  8336731  RESPONSABLE DU DOMAINE DATA & BI F/H - Exploit...   \n",
       "148  8335556  Data engineer F/H - Système, réseaux, données ...   \n",
       "149  8334817  ALTERNANCE - DATA MANAGEMENT - SEPTEMBRE 2024 ...   \n",
       "\n",
       "                                           description  \\\n",
       "0    METALINE, ESN spécialisée dans le recrutement ...   \n",
       "1    TA MISSION :\\nRattaché à l'une de nos agences,...   \n",
       "2    Dans le cadre de développement de ses activité...   \n",
       "3    Dans le cadre de notre développement, nous rec...   \n",
       "4    Si vous pensez que la Data et l'Intelligence A...   \n",
       "..                                                 ...   \n",
       "145  Descriptif du poste:\\n\\n                \\nEN B...   \n",
       "146  Vous rejoignez l'équipe IT dans laquelle vous ...   \n",
       "147  Descriptif du poste:\\n\\n \\n\\nEn tant que Respo...   \n",
       "148  Descriptif du poste:\\n\\nLe profil sélectionné ...   \n",
       "149  Descriptif du poste:\\n\\nRattaché au Manager DA...   \n",
       "\n",
       "                 dateCreation  \\\n",
       "0    2023-12-29T09:54:37.000Z   \n",
       "1    2023-12-28T16:50:04.000Z   \n",
       "2    2023-12-27T09:40:34.000Z   \n",
       "3    2023-12-22T11:56:51.000Z   \n",
       "4    2023-12-21T17:10:08.000Z   \n",
       "..                        ...   \n",
       "145  2023-12-21T09:45:12.000Z   \n",
       "146  2023-12-21T09:43:14.000Z   \n",
       "147  2023-12-20T08:57:53.000Z   \n",
       "148  2023-12-20T08:50:55.000Z   \n",
       "149  2023-12-20T08:46:40.000Z   \n",
       "\n",
       "                                           lieuTravail  \\\n",
       "0    {'libelle': '28 - CHARTRES', 'latitude': 48.44...   \n",
       "1    {'libelle': '59 - LILLE', 'latitude': 50.63099...   \n",
       "2    {'libelle': '92 - COURBEVOIE', 'latitude': 48....   \n",
       "3    {'libelle': '69 - LYON 06', 'latitude': 45.772...   \n",
       "4    {'libelle': '75 - PARIS 09', 'latitude': 48.87...   \n",
       "..                                                 ...   \n",
       "145  {'libelle': '33 - CENON', 'latitude': 44.85493...   \n",
       "146  {'libelle': '73 - CHAMBERY', 'latitude': 45.58...   \n",
       "147  {'libelle': '972 - LE LAMENTIN', 'latitude': 1...   \n",
       "148  {'libelle': '75 - PARIS 01', 'latitude': 48.85...   \n",
       "149  {'libelle': '85 - POUZAUGES', 'latitude': 46.7...   \n",
       "\n",
       "                                    appellationlibelle  \\\n",
       "0    Adjoint(e) technicien(ne) d'exploitation infor...   \n",
       "1         Analyste décisionnel - Business Intelligence   \n",
       "2    Consultant / Consultante en système d'information   \n",
       "3                                        Product Owner   \n",
       "4                                         Data manager   \n",
       "..                                                 ...   \n",
       "145                                       Data manager   \n",
       "146                                       Data manager   \n",
       "147             Responsable de domaine en informatique   \n",
       "148  Administrateur / Administratrice de plateforme...   \n",
       "149                           Responsable informatique   \n",
       "\n",
       "                                            entreprise typeContrat  \\\n",
       "0    {'nom': 'METALINE', 'description': 'METALOGIC ...         CDI   \n",
       "1    {'nom': 'NEXT DECISION', 'description': 'Fondé...         CDI   \n",
       "2    {'nom': 'KWET CONSEIL', 'description': 'Sociét...         CDI   \n",
       "3    {'nom': 'PROXIAD', 'description': 'Créée en 19...         CDI   \n",
       "4                         {'entrepriseAdaptee': False}         CDI   \n",
       "..                                                 ...         ...   \n",
       "145  {'nom': 'Adsearch', 'description': 'Adsearch v...         CDI   \n",
       "146  {'nom': 'Hays France', 'description': 'Notre c...         CDI   \n",
       "147  {'nom': 'INFOBAM', 'description': ' \n",
       "\n",
       "GBH empl...         CDI   \n",
       "148  {'nom': 'INGENIANCE', 'description': 'Depuis 2...         CDI   \n",
       "149  {'nom': 'FLEURY MICHON', 'description': 'Pourq...         CDD   \n",
       "\n",
       "                experienceLibelle  \\\n",
       "0                          6 mois   \n",
       "1                           3 ans   \n",
       "2                           2 ans   \n",
       "3                           3 ans   \n",
       "4                Débutant accepté   \n",
       "..                            ...   \n",
       "145  Expérience exigée de 2 An(s)   \n",
       "146  Expérience exigée de 5 An(s)   \n",
       "147  Expérience exigée de 5 An(s)   \n",
       "148  Expérience exigée de 1 An(s)   \n",
       "149              Débutant accepté   \n",
       "\n",
       "                                           competences  \\\n",
       "0    [{'code': '102175', 'libelle': 'Réaliser la ma...   \n",
       "1    [{'code': '109882', 'libelle': 'SQL', 'exigenc...   \n",
       "2    [{'code': '109818', 'libelle': 'Élaborer des s...   \n",
       "3    [{'code': '109818', 'libelle': 'Élaborer des s...   \n",
       "4    [{'code': '300195', 'libelle': 'Concevoir un l...   \n",
       "..                                                 ...   \n",
       "145                                                NaN   \n",
       "146                                                NaN   \n",
       "147                                                NaN   \n",
       "148                                                NaN   \n",
       "149                                                NaN   \n",
       "\n",
       "                                               salaire  alternance  \\\n",
       "0    {'commentaire': 'Selon profils', 'complement1'...       False   \n",
       "1    {'libelle': 'Annuel de 32000,00 Euros à 45000,...       False   \n",
       "2    {'libelle': 'Annuel de 35000,00 Euros à 38000,...       False   \n",
       "3    {'libelle': 'Annuel de 25000,00 Euros à 45000,...       False   \n",
       "4    {'libelle': 'Annuel de 45000,00 Euros à 55000,...       False   \n",
       "..                                                 ...         ...   \n",
       "145          {'commentaire': '40 - 50 k€ brut annuel'}       False   \n",
       "146  {'libelle': 'Annuel de 45000,00 Euros à 55000,...       False   \n",
       "147                      {'commentaire': 'A négocier'}       False   \n",
       "148          {'commentaire': '40 - 44 k€ brut annuel'}       False   \n",
       "149          {'commentaire': '22 - 25 k€ brut annuel'}        True   \n",
       "\n",
       "                                secteurActiviteLibelle  \n",
       "0       Conseil en systèmes et logiciels informatiques  \n",
       "1       Conseil en systèmes et logiciels informatiques  \n",
       "2    Conseil pour les affaires et autres conseils d...  \n",
       "3       Conseil en systèmes et logiciels informatiques  \n",
       "4                        Ingénierie, études techniques  \n",
       "..                                                 ...  \n",
       "145  Activités des agences de placement de main-d'œ...  \n",
       "146            Autres transports routiers de voyageurs  \n",
       "147                     Autres activités informatiques  \n",
       "148     Conseil en systèmes et logiciels informatiques  \n",
       "149                      Fabrication de plats préparés  \n",
       "\n",
       "[150 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jobs=df_resultat[colonnes_a_garder]\n",
    "df_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e389a688-27ff-413d-8b82-eab03889f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs40=df_jobs.loc[:20] #get the forty first rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b6a8b75-52a7-4107-9aab-1097415f268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs40_list=df_jobs40.T.to_dict().values() #transpose the DataFrame, convert into dictonnary and display the values of the dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6635cd85-943f-42d3-9015-cac32ad85306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "14770d98-e441-4463-b6f2-ec183eec73fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Je vais te fournir des offres d'emploi. Certaines sont en Français et d'autres et Anglais. Pour chacun de ces offres, tu devras extraire uniquement:\n",
    "- Les missions à effectuer(réduites, courtes)\n",
    "- les compétences techniques(décomposées en un mot-clé par ligne si possible) requises\n",
    "- les compétences techniques(décomposées en un mot-clé par ligne si possible) souhaitables ou appréciées sans être requises\n",
    "- les compétences non techniques(décomposées en un mot-clé par ligne si possible) attendues requises\n",
    "- Outils et technologies à maîtriser\n",
    "\n",
    "J'ai besoin de pouvoir structure et sauvegarder ces informations dans une base de données, d'avoir des libellés communs pour plusieurs façons d'exprimer une compétences, etc..\n",
    "Il est donc très important de les réduire aux maximum(pas de phrases).\n",
    "Les missions peuvent contenir des verbes d'actions mais rester très court et décomposés(une compétence à la fois par ligne, pas de \"et\")\n",
    "Les compétences et outils doivent être des mots clés dans la mesure du possible avec un mot-clé par ligne. Il faut gérer la séparation de mots clés si dans un annonce on en trouve plusieurs su la même ligne\n",
    "J'aimerais que le résultat soit sous la forme d'un JSON avec les clés: missions, required_technical_skills, desirable_technical_skills, required_non_technical_skills, tools_and_technologies.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9588dc7a-ccd5-43de-b27c-a57f9e6e897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6af2efd-0c46-4e7b-bc63-4f16c0075746",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_job_description(job_offer: str, debug=False) -> dict:\n",
    "    extracted_info = {}\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": job_offer}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "\n",
    "    text_response = response.choices[2].message.content\n",
    "    \n",
    "    try:\n",
    "        extracted_info = json.loads(text_response)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Could not parse AI's response into structured data.\")\n",
    "        if debug:\n",
    "            print(text_response)\n",
    "        return\n",
    "\n",
    "    return extracted_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd183726-cbfd-4c33-8f8e-3aa5e5c72b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'missions': [\"Gérer et suivre les alertes sur le réseau d'accès fixe\",\n",
       "  'Vérifier en temps réel les informations et savoir corréler les alertes avec les évènements ou les incidents en cours',\n",
       "  'Relancer par mail et/ou par téléphone les différents interlocuteurs',\n",
       "  \"Créer les tickets et assurer le pilotage technique jusqu'à la résolution\"],\n",
       " 'required_technical_skills': ['Support réseau télécom fixe',\n",
       "  'Informatique réseau',\n",
       "  'Routeurs IP',\n",
       "  'Protocole de routage',\n",
       "  'Connaissances des architectures réseaux fixes et mobiles',\n",
       "  'Connaissances IP',\n",
       "  'Boucle local (raccordement, optique, FTTx et DSL)'],\n",
       " 'desirable_technical_skills': [],\n",
       " 'required_non_technical_skills': ['Bonne communication orale et écrite',\n",
       "  'Capacité de formalisation',\n",
       "  'Capacité à accompagner et à former',\n",
       "  'Pédagogue et formateur',\n",
       "  'Rigueur, autonomie, respect des process',\n",
       "  \"Esprit d'analyse, résolution de problème\",\n",
       "  'Bon relationnel'],\n",
       " 'tools_and_technologies': ['Connaissances du contexte SFR']}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_jobs =process_job_description(job['description'], debug=True)\n",
    "extracted_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "49edd4a9-0426-4104-921b-09dd4421dc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {}\n",
      "2 {'id': '166XMTR'}\n",
      "3 {'id': '166XMTR', 'intitule': 'Technicien backup Data Center (H/F)'}\n",
      "4 {'id': '166XMTR', 'intitule': 'Technicien backup Data Center (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Installation/désinstallation de matériel', 'Câblage/Brassage entre équipements', 'Installation logiciels/paramétrages/firmware', \"Mise en service d'équipements\", 'Contrôle de liens/câbles via Fluke', 'Interventions sur incident, aide au diagnostic, vérifications', 'Gestions de médias de sauvegarde', \"Remplacement d'équipement\", 'Interventions spécifiques sous pilotage', \"Contrôle de l'environnement technique\", \"Gestion des demandes d'accès au site/accompagnement/contrôle suite à intervention\", \"Gestion des stocks de matériel sur les sites ne disposant pas d'un logisticien dédié (réception, livraison, stockage, transport)\", 'Tri sélectif dans le respect de la norme 14001', 'Accueil téléphonique en HNO', 'Exécution des demandes de travaux programmées en HNO', 'Compte-rendu de passage de consigne', 'Tenue de la position AMI/2AI (respectivement Normandie et Aubervilliers) : accueil, pilotage et traçage des activités effectuées', \"Participer si nécessaire aux rotations d'astreinte des sites\", \"Escalader à son Pilote d'Activité toute difficulté rencontrée dans l'exercice de ses fonctions\"], 'required_technical_skills': [], 'desirable_technical_skills': [], 'required_non_technical_skills': [], 'tools_and_technologies': ['Fluke', 'outils de suivi des demandes de travaux']}}\n",
      "\n",
      "\n",
      "1 {}\n",
      "2 {'id': '166XFCD'}\n",
      "3 {'id': '166XFCD', 'intitule': 'TALENTUEUX CONSULTANT BI / BIG DATA - LILLE - (H/F)'}\n",
      "4 {'id': '166XFCD', 'intitule': 'TALENTUEUX CONSULTANT BI / BIG DATA - LILLE - (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Apporter son expertise et son énergie sur des projets décisionnels et Big Data', 'Travailler sur des technologies innovantes et des secteurs divers', 'Intervenir en Conseil, Réalisation, Expertise, Formation et Développement'], 'required_technical_skills': ['Informatique décisionnelle', 'ETL / ELT : Talend DI, Semarchy xDI, MyReport Data', 'Dataviz : Power BI, Qlik Sense, MyReport Builder'], 'desirable_technical_skills': [], 'required_non_technical_skills': [], 'tools_and_technologies': []}}\n",
      "\n",
      "\n",
      "1 {}\n",
      "2 {'id': '166VKBR'}\n",
      "3 {'id': '166VKBR', 'intitule': 'Consultant Oracle Data Integration (H/F)'}\n",
      "4 {'id': '166VKBR', 'intitule': 'Consultant Oracle Data Integration (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Livraison des solutions de Business Intelligence (Microsoft et Oracle) dans le secteur à nos clients en Europe et au Moyen-Orient', 'Résolution des problèmes complexes des clients en donnant des conseils et en recommandant des solutions', 'Définition des architectures de solutions détaillées pour les solutions de services publics', 'Collaboration avec les intégrateurs de systèmes partenaires Oracle pour fournir des solutions clients', \"Collaboration directe avec les clients pour évaluer leurs besoins en matière de business intelligence, identifier les opportunités d'amélioration et développer des stratégies pour offrir une valeur mesurable\", \"Conception des solutions de business intelligence qui répondent aux exigences des clients, en utilisant une gamme d'outils et de technologies pour développer des solutions efficaces et efficientes\"], 'required_technical_skills': ['Oracle Data Integrator (ODI)', 'Microsoft SSRS SQL Server Reporting Services', 'Oracle SQL scripting and tuning skills', 'Oracle PL / SQL programming', 'Configuration (ReportServer.config)', 'Report Builder', 'Visual Basic', 'Visual Studio or Shell Scripting', 'SQL Server SQL and T-SQL', 'RS.exe command line utility', 'SQL Server command line scripts', 'SSIS (SQL Server integration services) packages', 'Command-Line Shell scripting including batch scripts', 'NET Powershell scripting'], 'desirable_technical_skills': [], 'required_non_technical_skills': [], 'tools_and_technologies': ['Oracle Data Integrator (ODI)', 'Microsoft SSRS SQL Server Reporting Services', 'Oracle SQL and PL/SQL', 'Visual Basic', 'Visual Studio', 'Shell Scripting', 'SQL Server', 'SSIS', 'Command-Line Shell scripting', 'NET Powershell scripting']}}\n",
      "\n",
      "\n",
      "1 {}\n",
      "2 {'id': '166SLVL'}\n",
      "3 {'id': '166SLVL', 'intitule': 'Product Owner Data (H/F)'}\n",
      "4 {'id': '166SLVL', 'intitule': 'Product Owner Data (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Définir et concevoir le produit', 'Porter la vision produit', 'Faire le lien entre le business/métier et la partie technique', 'Rédiger les User Stories', 'Préparer et suivre les mises en recette et les mises en production', 'Faire le bilan de la recette', 'Préparer les phases de démos', 'Préparer le prochain PI avec les différents interlocuteurs', 'Répondre aux différentes sollicitations', \"Construire l'argumentation pour les arbitrages\", 'Participer activement à la rétro', \"Collecter les business values réalisées sur l'incrément\", 'Tenir des réunions de suivi avec les partenaires amont/aval'], 'required_technical_skills': ['Jira', 'Confluence'], 'desirable_technical_skills': ['Miro', 'iObeya'], 'required_non_technical_skills': ['Expérience professionnelle en tant que Product Owner Data', 'Connaissances en cloud (GCP ou autres)', 'Connaissances en Big Data', 'Expérience en projets de cartographie', 'Culture Data Science'], 'tools_and_technologies': ['Jira', 'Confluence', 'Miro', 'iObeya', 'GCP', 'Big Data']}}\n",
      "\n",
      "\n",
      "1 {}\n",
      "2 {'id': '166RQMR'}\n",
      "3 {'id': '166RQMR', 'intitule': 'Data Engineer (H/F)'}\n",
      "4 {'id': '166RQMR', 'intitule': 'Data Engineer (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': [\"Intégrer une communauté d'experts Data passionnés\", 'Recevoir et partager de la connaissance et des savoirs-faire lors de nombreux évènements', \"Intervenir chez des clients pour y porter l'expertise Wewyse dans des contextes et des secteurs variés\", 'Participer à des projets innovants au sein de notre Datalab, avec des Wysers mais aussi avec des partenaires académiques et des start-up', \"Viser l'excellence des développements en s'appuyant sur le Software craftsmanship\", 'Concevoir des architectures logicielles modernes', \"Penser DevOps pour l'automatisation des déploiements et la continuité des services\", 'Être encouragé, conseillé et accompagné dans un parcours de formation adapté à vos ambitions professionnelles'], 'required_technical_skills': [], 'desirable_technical_skills': [], 'required_non_technical_skills': [], 'tools_and_technologies': []}}\n",
      "\n",
      "\n",
      "1 {}\n",
      "2 {'id': '166RBJS'}\n",
      "3 {'id': '166RBJS', 'intitule': 'Technicien Data Centre Senior H/F (H/F)'}\n",
      "4 {'id': '166RBJS', 'intitule': 'Technicien Data Centre Senior H/F (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': [\"Planifier et organiser l'équipe de support\", \"Gérer les besoins en formations de l'équipe, la planification et l'organisation\", 'Evaluer la qualité des formations suivies et reporter au responsable de contrat', 'Planifier et organiser les congés', 'Mettre en place et assurer le contrôle qualité', \"Editer les rapports d'activités\", \"Identifier et proposer des axes d'amélioration\", \"Gérer l'infrastructure des serveurs (mise en place - décommissionnement)\", 'Installer le matériel et les équipements dans le data centre', 'Réaliser le câblage', \"Veiller au bon fonctionnement des équipements, à l'accessibilité et la sécurité des données\", 'Diagnostiquer toutes pannes hardware ou software', 'Changer les pièces défectueuses', 'Gérer le stock de pièces détachées', 'Gérer les commandes et suivis avec les fournisseurs de pièces détachées et de serveurs', \"Gérer des incidents et requêtes dans l'outil Service Now\", 'Appliquer rigoureusement les procédures mises en place par le client', 'Participer aux réunions avec le client'], 'required_technical_skills': [\"gestion d'équipe\", 'installation de matériel', 'câblage', 'diagnostic de pannes', 'changement de pièces défectueuses', 'gestion de stock', \"gestion d'incidents\", 'Service Now', 'procédures client'], 'desirable_technical_skills': [], 'required_non_technical_skills': [], 'tools_and_technologies': ['serveurs', 'data centre']}}\n",
      "\n",
      "\n",
      "1 {}\n",
      "2 {'id': '166PTQP'}\n",
      "3 {'id': '166PTQP', 'intitule': 'Referential Data Manager (H/F)'}\n",
      "4 {'id': '166PTQP', 'intitule': 'Referential Data Manager (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Gérer les listes de valeur sur le périmètre de données prioritaire', 'Cadrer et spécifier les mécanismes de gestion de référentiels', 'Accompagner la mise en place des mécanismes validés'], 'required_technical_skills': ['MDM', 'SQL'], 'desirable_technical_skills': [], 'required_non_technical_skills': ['Rigueur', 'Pragmatisme', 'Pédagogie', \"Esprit d'analyse et de synthèse\", 'Bon relationnel', \"Esprit d'équipe\"], 'tools_and_technologies': ['Langage SQL']}}\n",
      "\n",
      "\n",
      "1 {}\n",
      "2 {'id': '166MPJT'}\n",
      "3 {'id': '166MPJT', 'intitule': 'Analyste en Business Intelligence et Data Science (H/F)'}\n",
      "4 {'id': '166MPJT', 'intitule': 'Analyste en Business Intelligence et Data Science (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Création de Reportings Analytics en utilisant des outils tels que Power BI et Tableau', 'Optimisation des KPIs et des Reportings existants en utilisant Python et R', 'Interrogation de bases de données variées comme MySQL, Postgresql et MongoDB', 'Création de rapports détaillés pour faciliter la prise de décision en utilisant Python et R', 'Utilisation de logiciels spécialisés comme SAS pour mener des analyses statistiques', 'Conception et développement de tableaux de bord interactifs en utilisant Power BI et Tableau', 'Utilisation efficace des outils MS Office, notamment Word et Excel, pour créer des documents et des rapports professionnels'], 'required_technical_skills': ['Power BI', 'Tableau', 'Python', 'R', 'MySQL', 'Postgresql', 'MongoDB', 'SAS', 'MS Office (Word, Excel)'], 'desirable_technical_skills': [], 'required_non_technical_skills': [], 'tools_and_technologies': ['Power BI', 'Tableau', 'Python', 'R', 'MySQL', 'Postgresql', 'MongoDB', 'SAS', 'MS Office (Word, Excel)']}}\n",
      "\n",
      "\n",
      "1 {}\n",
      "2 {'id': '166LZBG'}\n",
      "3 {'id': '166LZBG', 'intitule': 'Data Engineer (H/F)'}\n",
      "4 {'id': '166LZBG', 'intitule': 'Data Engineer (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Mise à disposition de données fiables', 'Exploitation des données', 'Application de la politique de gouvernance des données', 'Développement de la plateforme Data', 'Diversification des sources de données', \"Garantir la fiabilité de l'alimentation des données\", \"Industrialisation du déploiement d'algorithmes de prédiction et de recommandation\", 'Construction de visions des données structurées', 'Maintenance et évolution des outils', 'Garantie de la disponibilité et de la qualité des données', \"Sensibilisation et partage des connaissances avec l'équipe\"], 'required_technical_skills': [], 'desirable_technical_skills': [], 'required_non_technical_skills': [], 'tools_and_technologies': ['ETL', 'Dataviz']}}\n",
      "\n",
      "\n",
      "1 {}\n",
      "2 {'id': '166LKCF'}\n",
      "3 {'id': '166LKCF', 'intitule': 'DATA ENGINEER - BUSINESS INTELLIGENCE (H/F)'}\n",
      "4 {'id': '166LKCF', 'intitule': 'DATA ENGINEER - BUSINESS INTELLIGENCE (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Concevoir, développer et maintenir des pipelines de données et des processus ETL pour ingérer, transformer et charger des données provenant de diverses sources financières dans notre plateforme de données.', 'Développer et optimiser des requêtes SQL, des scripts PySpark, des calculs DAX et des transformations Power Query M pour extraire et manipuler des données.', 'Utiliser efficacement les services et outils Azure pour construire des solutions de données évolutives et performantes.', \"Travailler en étroite collaboration avec l'équipe de gestion des données pour soutenir la gouvernance des données et garantir le respect des normes et des meilleures pratiques en matière de données.\", \"Superviser l'élaboration et la mise en œuvre d'un plan d'action pour l'amélioration de la qualité de l'eau et de l'assainissement.\", \"Contribuer à la livraison et au succès d'un portefeuille de projets, y compris des tâches backend telles que l'extraction de données, l'ingestion, la préparation de tables et la transformation de données, ainsi que des tâches front-end telles que les aspects de visualisation sur Power BI.\", \"Documenter clairement et complètement les processus d'ingénierie des données.\"], 'required_technical_skills': ['ETL', 'SQL', 'PySpark', 'DAX', 'Power Query M', 'Azure', 'Data Governance'], 'desirable_technical_skills': ['Finance', 'Assurance', 'Shine', 'Azure DevOps'], 'required_non_technical_skills': ['Conception', 'Analyse', 'Gestion de projet'], 'tools_and_technologies': ['Azure Devops', 'Azure', 'Power BI', 'Shine']}}\n",
      "\n",
      "\n",
      "1 {}\n",
      "2 {'id': '166LJVL'}\n",
      "3 {'id': '166LJVL', 'intitule': 'DATA ENGINEER - BUSINESS INTELLIGENCE (H/F)'}\n",
      "4 {'id': '166LJVL', 'intitule': 'DATA ENGINEER - BUSINESS INTELLIGENCE (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Concevoir, développer et maintenir des pipelines de données et des processus ETL', 'Développer et optimiser des requêtes SQL, des scripts PySpark, des calculs DAX et des transformations Power Query M', 'Utiliser efficacement les services et outils Azure pour construire des solutions de données évolutives et performantes', \"Travailler en étroite collaboration avec l'équipe de gestion des données pour soutenir la gouvernance des données\", \"Superviser l'élaboration et la mise en œuvre d'un plan d'action pour l'amélioration de la qualité de l'eau et de l'assainissement\", \"Contribuer à la livraison et au succès d'un portefeuille de projets, y compris des tâches backend et front-end\", \"Documenter clairement et complètement les processus d'ingénierie des données\"], 'required_technical_skills': ['ETL', 'SQL', 'PySpark', 'DAX', 'Power Query M', 'Azure', 'Gestion des données'], 'desirable_technical_skills': ['Finance', 'Assurance', 'Azure Devops', 'Shine', 'Power BI'], 'required_non_technical_skills': [], 'tools_and_technologies': ['ETL', 'SQL', 'PySpark', 'DAX', 'Power Query M', 'Azure', 'Azure Devops', 'Shine', 'Power BI']}}\n",
      "\n",
      "\n",
      "1 {}\n",
      "2 {'id': '166KMWH'}\n",
      "3 {'id': '166KMWH', 'intitule': 'DATA ARCHITECT (H/F)'}\n",
      "4 {'id': '166KMWH', 'intitule': 'DATA ARCHITECT (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Conseil, expertise et audits techniques', 'Cartographie des processus', \"Définition de Roadmaps et d'architectures Data\", \"Aide au choix d'outils\", 'PoC/MVP', 'Coaching/formations', 'Participation au recrutement et à la formation des talents data', 'Veille technologique et animation de la communauté data'], 'required_technical_skills': ['Architectures Data', 'ETL', 'Outil de dataviz'], 'desirable_technical_skills': [], 'required_non_technical_skills': [\"Esprit d'équipe\", 'Autonomie', 'Curiosité technique', \"Sens de l'écoute\", \"Capacité d'adaptation\", 'Communication'], 'tools_and_technologies': []}}\n",
      "\n",
      "\n",
      "1 {}\n",
      "2 {'id': '166KMSV'}\n",
      "3 {'id': '166KMSV', 'intitule': 'RECHERCHE DATA MANAGER - (H/F)'}\n",
      "Could not parse AI's response into structured data.\n",
      "4 {'id': '166KMSV', 'intitule': 'RECHERCHE DATA MANAGER - (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': None}\n",
      "\n",
      "\n",
      "1 {}\n",
      "2 {'id': '166KMJZ'}\n",
      "3 {'id': '166KMJZ', 'intitule': 'Ingénieur Big Data H/F'}\n",
      "4 {'id': '166KMJZ', 'intitule': 'Ingénieur Big Data H/F', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Intégration de solution big data', 'Développement des modèles de données', 'Développement des scripts de traitements', 'Automatisation des déploiements', 'Réalisation des maintenances évolutive / corrective'], 'required_technical_skills': ['Linux (Redhat)', 'Scripting : Python / PySpark', 'Automation (Ansible)', 'Data : Apache Airflow / Kafka / ElasticSearch / Spark / Nifi'], 'desirable_technical_skills': [], 'required_non_technical_skills': ['Organisation', 'Polyvalence', 'Rigueur', 'Curiosité'], 'tools_and_technologies': []}}\n",
      "\n",
      "\n",
      "1 {}\n",
      "2 {'id': '166JWRH'}\n",
      "3 {'id': '166JWRH', 'intitule': 'Ingénieur Data Intégration Sénior (H/F)'}\n",
      "4 {'id': '166JWRH', 'intitule': 'Ingénieur Data Intégration Sénior (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Analyser techniquement et fonctionnellement le besoin', 'Participer à la rédaction de spécifications techniques et aux procédures de livraison', 'Concevoir des procédures de transformation de données (maping)', 'Migrer et/ou développer des flux de données de type ETL/ELT/ESB', \"Développer des tests applicatifs, définir et réaliser les procédures de tests techniques, de tests d'intégration et de recette de ces développements\", 'Participer à la conception et mise en place de datawarehouse', 'Encadrer des développeurs juniors', 'Prendre en charge la maintenance évolutive et corrective en conditions opérationnelles', \"Mettre en place et faire respecter les normes de développement et d'architecture définies\"], 'required_technical_skills': [\"flux d'alimentation\", 'traitement de données', 'Talend', 'Informatica', 'Stambia', 'Datastage Server', 'Oracle Data Integrator (ODI)', 'Tibco Jaspersoft', 'Azure Data Factory', 'Pentaho', 'SQL/PL-SQL', 'Java', 'bases de données on-premise et/ou cloud'], 'desirable_technical_skills': ['Talend Enterprise Data Intégration'], 'required_non_technical_skills': ['capacités relationnelles', 'rédactionnelles', 'analytiques', \"sens de l'écoute\", 'communication', \"capacité d'adaptation\", \"esprit d'équipe\"], 'tools_and_technologies': ['Talend', 'Informatica', 'Stambia', 'Datastage Server', 'Oracle Data Integrator (ODI)', 'Tibco Jaspersoft', 'Azure Data Factory', 'Pentaho', 'SQL', 'PL-SQL', 'Java']}}\n",
      "\n",
      "\n",
      "1 {}\n",
      "2 {'id': '166JRCT'}\n",
      "3 {'id': '166JRCT', 'intitule': 'Senior Data Engineer (H/F)'}\n",
      "Could not parse AI's response into structured data.\n",
      "4 {'id': '166JRCT', 'intitule': 'Senior Data Engineer (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': None}\n",
      "\n",
      "\n",
      "1 {}\n",
      "2 {'id': '166HXCK'}\n",
      "3 {'id': '166HXCK', 'intitule': 'Data manager'}\n",
      "4 {'id': '166HXCK', 'intitule': 'Data manager', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Validation des spécifications générales et détaillées des produits et solutions', \"Animation réunion/échange avec les architectes de la donnée et le gestionnaire de l'offre marketing\", 'Conception technique détaillée des modèles pour la configuration', 'Implémentation de la solution technique et son intégration dans le flux E2E SAP', 'Élaboration des plans de tests unitaires et fonctionnels', 'Déploiement des produits et de leurs paramétrages dans les environnements de Ventes et Production', 'Suivi et maintenance des données de configurations', 'Support aux utilisateurs Front et Backoffice'], 'required_technical_skills': ['SAP ERP Administration des ventes (SD) / Production Planning (PP)', 'Variant Configurator (LO-VC)', 'Sales Solution Configurator (SSC)', \"Outils d'import/export et de transformation de données\", 'Netweaver Eclipse', 'SAP-SME (Solution Modeling Environment)', 'SAP NetWeaver 7.5 Stack Java /ECC 6.0', 'Amazon Web Services (AWS)', 'Cloud'], 'desirable_technical_skills': ['Python', 'Java', 'ABAP', 'langage de programmation sous contrainte'], 'required_non_technical_skills': ['Anglais niveau écrit/parlé courant (niveau B2 mini) dans un environnement technique/industriel', \"Bon sens de l'organisation\", 'Travail en équipe', 'Relationnel'], 'tools_and_technologies': ['SAP ERP Administration des ventes (SD)', 'Production Planning (PP)', 'Variant Configurator (LO-VC)', 'Sales Solution Configurator (SSC)', 'Netweaver Eclipse', 'SAP-SME (Solution Modeling Environment)', 'SAP NetWeaver 7.5', 'ECC 6.0', 'Amazon Web Services (AWS)', 'Cloud']}}\n",
      "\n",
      "\n",
      "1 {}\n",
      "2 {'id': '166HTRW'}\n",
      "3 {'id': '166HTRW', 'intitule': 'Technicien backup Data Center (H/F)'}\n",
      "4 {'id': '166HTRW', 'intitule': 'Technicien backup Data Center (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': [\"Rackage, installation équipements, brassage, câblage, inventaire, mise à jour au niveau d'outils\", 'Accompagnement et montée en compétences', 'Adaptation aux dernières nouveautés et capacité à innover'], 'required_technical_skills': [], 'desirable_technical_skills': [], 'required_non_technical_skills': [], 'tools_and_technologies': []}}\n",
      "\n",
      "\n",
      "1 {}\n",
      "2 {'id': '166HLYZ'}\n",
      "3 {'id': '166HLYZ', 'intitule': 'DATA ENGINEER (H/F)'}\n",
      "4 {'id': '166HLYZ', 'intitule': 'DATA ENGINEER (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': [\"Concevoir et réaliser une solution Data de l'acquisition à l'exploitation de la donnée\", 'Collecter et intégrer les données nécessaires à la résolution de problématiques métier et opérationnelles', 'Garantir la qualité des données en appliquant les règles de Data Gouvernance et de Data Management', 'Transcrire les besoins métier en règles de gestion data', \"Industrialiser le déploiement des réalisations avec des tests unitaires, d'intégration et de non-régression\"], 'required_technical_skills': ['Conception de solution Data', 'Collecte et intégration de données', 'Data Gouvernance', 'Data Management'], 'desirable_technical_skills': [], 'required_non_technical_skills': [], 'tools_and_technologies': ['Business intelligence', 'Data Analyse', 'Développement orienté objet']}}\n",
      "\n",
      "\n",
      "1 {}\n",
      "2 {'id': '166HFTP'}\n",
      "3 {'id': '166HFTP', 'intitule': 'Chef ou cheffe de projet Big Data (H/F)'}\n",
      "4 {'id': '166HFTP', 'intitule': 'Chef ou cheffe de projet Big Data (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': [\"Piloter un projet BIG DATA d'ingestion et de traitement de données\", \"Produire des reportings et analyses Adhoc sur l'analyse des sillons, les impacts des plages de travaux, la relation contractuelle avec le gestionnaire d'infrastructure\", 'Animer et produire les comptes-rendus des comités', \"Suivre l'avancement des travaux de l'équipe DSI\", 'Piloter les opérations de suivi de run et gérer les incidents de production', 'Produire le reporting à destination de la MOA et du responsable de domaine', 'Mettre en place et suivre les différents outils de pilotage', 'Tenir à jour les outils de suivi économique'], 'required_technical_skills': ['Big Data', 'Architecture', 'Sécurité', 'Performance'], 'desirable_technical_skills': ['Éco-conception'], 'required_non_technical_skills': ['Animation', 'Gestion de projet', 'Comptes-rendus', \"Suivi d'avancement\", 'Pilotage opérationnel', 'Reporting', \"Gestion d'incidents\", 'Pilotage économique'], 'tools_and_technologies': ['JIRA']}}\n",
      "\n",
      "\n",
      "1 {}\n",
      "2 {'id': '166GTKV'}\n",
      "3 {'id': '166GTKV', 'intitule': 'Chargé de production performance Data Accès Fixe (H/F)'}\n",
      "4 {'id': '166GTKV', 'intitule': 'Chargé de production performance Data Accès Fixe (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': [\"Gérer et suivre les alertes sur le réseau d'accès fixe\", 'Vérifier en temps réel les informations et savoir corréler les alertes avec les évènements ou les incidents en cours', 'Relancer par mail et/ou par téléphone les différents interlocuteurs', \"Créer les tickets et assurer le pilotage technique jusqu'à la résolution\"], 'required_technical_skills': ['Support réseau télécom fixe', 'Informatique réseau', 'Routeurs IP', 'Protocole de routage', 'Connaissances des architectures réseaux fixes et mobiles', 'Connaissances IP', 'Profil Telecom', 'Expérience boucle local', 'Raccordement', 'Optique', 'FTTx', 'DSL'], 'desirable_technical_skills': [], 'required_non_technical_skills': ['Bonne communication orale et écrite', 'Capacité de formalisation', 'Capacité à accompagner et à former', 'Pédagogue et formateur', 'Rigueur, autonomie, respect des process', \"Esprit d'analyse, résolution de problème\", 'Bon relationnel'], 'tools_and_technologies': ['Routeurs IP', 'Protocole de routage', 'Connaissances des architectures réseaux fixes et mobiles', 'Connaissances IP', 'Raccordement', 'Optique', 'FTTx', 'DSL']}}\n",
      "\n",
      "\n",
      "[{'id': '166XMTR', 'intitule': 'Technicien backup Data Center (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Installation/désinstallation de matériel', 'Câblage/Brassage entre équipements', 'Installation logiciels/paramétrages/firmware', \"Mise en service d'équipements\", 'Contrôle de liens/câbles via Fluke', 'Interventions sur incident, aide au diagnostic, vérifications', 'Gestions de médias de sauvegarde', \"Remplacement d'équipement\", 'Interventions spécifiques sous pilotage', \"Contrôle de l'environnement technique\", \"Gestion des demandes d'accès au site/accompagnement/contrôle suite à intervention\", \"Gestion des stocks de matériel sur les sites ne disposant pas d'un logisticien dédié (réception, livraison, stockage, transport)\", 'Tri sélectif dans le respect de la norme 14001', 'Accueil téléphonique en HNO', 'Exécution des demandes de travaux programmées en HNO', 'Compte-rendu de passage de consigne', 'Tenue de la position AMI/2AI (respectivement Normandie et Aubervilliers) : accueil, pilotage et traçage des activités effectuées', \"Participer si nécessaire aux rotations d'astreinte des sites\", \"Escalader à son Pilote d'Activité toute difficulté rencontrée dans l'exercice de ses fonctions\"], 'required_technical_skills': [], 'desirable_technical_skills': [], 'required_non_technical_skills': [], 'tools_and_technologies': ['Fluke', 'outils de suivi des demandes de travaux']}}, {'id': '166XFCD', 'intitule': 'TALENTUEUX CONSULTANT BI / BIG DATA - LILLE - (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Apporter son expertise et son énergie sur des projets décisionnels et Big Data', 'Travailler sur des technologies innovantes et des secteurs divers', 'Intervenir en Conseil, Réalisation, Expertise, Formation et Développement'], 'required_technical_skills': ['Informatique décisionnelle', 'ETL / ELT : Talend DI, Semarchy xDI, MyReport Data', 'Dataviz : Power BI, Qlik Sense, MyReport Builder'], 'desirable_technical_skills': [], 'required_non_technical_skills': [], 'tools_and_technologies': []}}, {'id': '166VKBR', 'intitule': 'Consultant Oracle Data Integration (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Livraison des solutions de Business Intelligence (Microsoft et Oracle) dans le secteur à nos clients en Europe et au Moyen-Orient', 'Résolution des problèmes complexes des clients en donnant des conseils et en recommandant des solutions', 'Définition des architectures de solutions détaillées pour les solutions de services publics', 'Collaboration avec les intégrateurs de systèmes partenaires Oracle pour fournir des solutions clients', \"Collaboration directe avec les clients pour évaluer leurs besoins en matière de business intelligence, identifier les opportunités d'amélioration et développer des stratégies pour offrir une valeur mesurable\", \"Conception des solutions de business intelligence qui répondent aux exigences des clients, en utilisant une gamme d'outils et de technologies pour développer des solutions efficaces et efficientes\"], 'required_technical_skills': ['Oracle Data Integrator (ODI)', 'Microsoft SSRS SQL Server Reporting Services', 'Oracle SQL scripting and tuning skills', 'Oracle PL / SQL programming', 'Configuration (ReportServer.config)', 'Report Builder', 'Visual Basic', 'Visual Studio or Shell Scripting', 'SQL Server SQL and T-SQL', 'RS.exe command line utility', 'SQL Server command line scripts', 'SSIS (SQL Server integration services) packages', 'Command-Line Shell scripting including batch scripts', 'NET Powershell scripting'], 'desirable_technical_skills': [], 'required_non_technical_skills': [], 'tools_and_technologies': ['Oracle Data Integrator (ODI)', 'Microsoft SSRS SQL Server Reporting Services', 'Oracle SQL and PL/SQL', 'Visual Basic', 'Visual Studio', 'Shell Scripting', 'SQL Server', 'SSIS', 'Command-Line Shell scripting', 'NET Powershell scripting']}}, {'id': '166SLVL', 'intitule': 'Product Owner Data (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Définir et concevoir le produit', 'Porter la vision produit', 'Faire le lien entre le business/métier et la partie technique', 'Rédiger les User Stories', 'Préparer et suivre les mises en recette et les mises en production', 'Faire le bilan de la recette', 'Préparer les phases de démos', 'Préparer le prochain PI avec les différents interlocuteurs', 'Répondre aux différentes sollicitations', \"Construire l'argumentation pour les arbitrages\", 'Participer activement à la rétro', \"Collecter les business values réalisées sur l'incrément\", 'Tenir des réunions de suivi avec les partenaires amont/aval'], 'required_technical_skills': ['Jira', 'Confluence'], 'desirable_technical_skills': ['Miro', 'iObeya'], 'required_non_technical_skills': ['Expérience professionnelle en tant que Product Owner Data', 'Connaissances en cloud (GCP ou autres)', 'Connaissances en Big Data', 'Expérience en projets de cartographie', 'Culture Data Science'], 'tools_and_technologies': ['Jira', 'Confluence', 'Miro', 'iObeya', 'GCP', 'Big Data']}}, {'id': '166RQMR', 'intitule': 'Data Engineer (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': [\"Intégrer une communauté d'experts Data passionnés\", 'Recevoir et partager de la connaissance et des savoirs-faire lors de nombreux évènements', \"Intervenir chez des clients pour y porter l'expertise Wewyse dans des contextes et des secteurs variés\", 'Participer à des projets innovants au sein de notre Datalab, avec des Wysers mais aussi avec des partenaires académiques et des start-up', \"Viser l'excellence des développements en s'appuyant sur le Software craftsmanship\", 'Concevoir des architectures logicielles modernes', \"Penser DevOps pour l'automatisation des déploiements et la continuité des services\", 'Être encouragé, conseillé et accompagné dans un parcours de formation adapté à vos ambitions professionnelles'], 'required_technical_skills': [], 'desirable_technical_skills': [], 'required_non_technical_skills': [], 'tools_and_technologies': []}}, {'id': '166RBJS', 'intitule': 'Technicien Data Centre Senior H/F (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': [\"Planifier et organiser l'équipe de support\", \"Gérer les besoins en formations de l'équipe, la planification et l'organisation\", 'Evaluer la qualité des formations suivies et reporter au responsable de contrat', 'Planifier et organiser les congés', 'Mettre en place et assurer le contrôle qualité', \"Editer les rapports d'activités\", \"Identifier et proposer des axes d'amélioration\", \"Gérer l'infrastructure des serveurs (mise en place - décommissionnement)\", 'Installer le matériel et les équipements dans le data centre', 'Réaliser le câblage', \"Veiller au bon fonctionnement des équipements, à l'accessibilité et la sécurité des données\", 'Diagnostiquer toutes pannes hardware ou software', 'Changer les pièces défectueuses', 'Gérer le stock de pièces détachées', 'Gérer les commandes et suivis avec les fournisseurs de pièces détachées et de serveurs', \"Gérer des incidents et requêtes dans l'outil Service Now\", 'Appliquer rigoureusement les procédures mises en place par le client', 'Participer aux réunions avec le client'], 'required_technical_skills': [\"gestion d'équipe\", 'installation de matériel', 'câblage', 'diagnostic de pannes', 'changement de pièces défectueuses', 'gestion de stock', \"gestion d'incidents\", 'Service Now', 'procédures client'], 'desirable_technical_skills': [], 'required_non_technical_skills': [], 'tools_and_technologies': ['serveurs', 'data centre']}}, {'id': '166PTQP', 'intitule': 'Referential Data Manager (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Gérer les listes de valeur sur le périmètre de données prioritaire', 'Cadrer et spécifier les mécanismes de gestion de référentiels', 'Accompagner la mise en place des mécanismes validés'], 'required_technical_skills': ['MDM', 'SQL'], 'desirable_technical_skills': [], 'required_non_technical_skills': ['Rigueur', 'Pragmatisme', 'Pédagogie', \"Esprit d'analyse et de synthèse\", 'Bon relationnel', \"Esprit d'équipe\"], 'tools_and_technologies': ['Langage SQL']}}, {'id': '166MPJT', 'intitule': 'Analyste en Business Intelligence et Data Science (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Création de Reportings Analytics en utilisant des outils tels que Power BI et Tableau', 'Optimisation des KPIs et des Reportings existants en utilisant Python et R', 'Interrogation de bases de données variées comme MySQL, Postgresql et MongoDB', 'Création de rapports détaillés pour faciliter la prise de décision en utilisant Python et R', 'Utilisation de logiciels spécialisés comme SAS pour mener des analyses statistiques', 'Conception et développement de tableaux de bord interactifs en utilisant Power BI et Tableau', 'Utilisation efficace des outils MS Office, notamment Word et Excel, pour créer des documents et des rapports professionnels'], 'required_technical_skills': ['Power BI', 'Tableau', 'Python', 'R', 'MySQL', 'Postgresql', 'MongoDB', 'SAS', 'MS Office (Word, Excel)'], 'desirable_technical_skills': [], 'required_non_technical_skills': [], 'tools_and_technologies': ['Power BI', 'Tableau', 'Python', 'R', 'MySQL', 'Postgresql', 'MongoDB', 'SAS', 'MS Office (Word, Excel)']}}, {'id': '166LZBG', 'intitule': 'Data Engineer (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Mise à disposition de données fiables', 'Exploitation des données', 'Application de la politique de gouvernance des données', 'Développement de la plateforme Data', 'Diversification des sources de données', \"Garantir la fiabilité de l'alimentation des données\", \"Industrialisation du déploiement d'algorithmes de prédiction et de recommandation\", 'Construction de visions des données structurées', 'Maintenance et évolution des outils', 'Garantie de la disponibilité et de la qualité des données', \"Sensibilisation et partage des connaissances avec l'équipe\"], 'required_technical_skills': [], 'desirable_technical_skills': [], 'required_non_technical_skills': [], 'tools_and_technologies': ['ETL', 'Dataviz']}}, {'id': '166LKCF', 'intitule': 'DATA ENGINEER - BUSINESS INTELLIGENCE (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Concevoir, développer et maintenir des pipelines de données et des processus ETL pour ingérer, transformer et charger des données provenant de diverses sources financières dans notre plateforme de données.', 'Développer et optimiser des requêtes SQL, des scripts PySpark, des calculs DAX et des transformations Power Query M pour extraire et manipuler des données.', 'Utiliser efficacement les services et outils Azure pour construire des solutions de données évolutives et performantes.', \"Travailler en étroite collaboration avec l'équipe de gestion des données pour soutenir la gouvernance des données et garantir le respect des normes et des meilleures pratiques en matière de données.\", \"Superviser l'élaboration et la mise en œuvre d'un plan d'action pour l'amélioration de la qualité de l'eau et de l'assainissement.\", \"Contribuer à la livraison et au succès d'un portefeuille de projets, y compris des tâches backend telles que l'extraction de données, l'ingestion, la préparation de tables et la transformation de données, ainsi que des tâches front-end telles que les aspects de visualisation sur Power BI.\", \"Documenter clairement et complètement les processus d'ingénierie des données.\"], 'required_technical_skills': ['ETL', 'SQL', 'PySpark', 'DAX', 'Power Query M', 'Azure', 'Data Governance'], 'desirable_technical_skills': ['Finance', 'Assurance', 'Shine', 'Azure DevOps'], 'required_non_technical_skills': ['Conception', 'Analyse', 'Gestion de projet'], 'tools_and_technologies': ['Azure Devops', 'Azure', 'Power BI', 'Shine']}}, {'id': '166LJVL', 'intitule': 'DATA ENGINEER - BUSINESS INTELLIGENCE (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Concevoir, développer et maintenir des pipelines de données et des processus ETL', 'Développer et optimiser des requêtes SQL, des scripts PySpark, des calculs DAX et des transformations Power Query M', 'Utiliser efficacement les services et outils Azure pour construire des solutions de données évolutives et performantes', \"Travailler en étroite collaboration avec l'équipe de gestion des données pour soutenir la gouvernance des données\", \"Superviser l'élaboration et la mise en œuvre d'un plan d'action pour l'amélioration de la qualité de l'eau et de l'assainissement\", \"Contribuer à la livraison et au succès d'un portefeuille de projets, y compris des tâches backend et front-end\", \"Documenter clairement et complètement les processus d'ingénierie des données\"], 'required_technical_skills': ['ETL', 'SQL', 'PySpark', 'DAX', 'Power Query M', 'Azure', 'Gestion des données'], 'desirable_technical_skills': ['Finance', 'Assurance', 'Azure Devops', 'Shine', 'Power BI'], 'required_non_technical_skills': [], 'tools_and_technologies': ['ETL', 'SQL', 'PySpark', 'DAX', 'Power Query M', 'Azure', 'Azure Devops', 'Shine', 'Power BI']}}, {'id': '166KMWH', 'intitule': 'DATA ARCHITECT (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Conseil, expertise et audits techniques', 'Cartographie des processus', \"Définition de Roadmaps et d'architectures Data\", \"Aide au choix d'outils\", 'PoC/MVP', 'Coaching/formations', 'Participation au recrutement et à la formation des talents data', 'Veille technologique et animation de la communauté data'], 'required_technical_skills': ['Architectures Data', 'ETL', 'Outil de dataviz'], 'desirable_technical_skills': [], 'required_non_technical_skills': [\"Esprit d'équipe\", 'Autonomie', 'Curiosité technique', \"Sens de l'écoute\", \"Capacité d'adaptation\", 'Communication'], 'tools_and_technologies': []}}, {'id': '166KMSV', 'intitule': 'RECHERCHE DATA MANAGER - (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': None}, {'id': '166KMJZ', 'intitule': 'Ingénieur Big Data H/F', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Intégration de solution big data', 'Développement des modèles de données', 'Développement des scripts de traitements', 'Automatisation des déploiements', 'Réalisation des maintenances évolutive / corrective'], 'required_technical_skills': ['Linux (Redhat)', 'Scripting : Python / PySpark', 'Automation (Ansible)', 'Data : Apache Airflow / Kafka / ElasticSearch / Spark / Nifi'], 'desirable_technical_skills': [], 'required_non_technical_skills': ['Organisation', 'Polyvalence', 'Rigueur', 'Curiosité'], 'tools_and_technologies': []}}, {'id': '166JWRH', 'intitule': 'Ingénieur Data Intégration Sénior (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Analyser techniquement et fonctionnellement le besoin', 'Participer à la rédaction de spécifications techniques et aux procédures de livraison', 'Concevoir des procédures de transformation de données (maping)', 'Migrer et/ou développer des flux de données de type ETL/ELT/ESB', \"Développer des tests applicatifs, définir et réaliser les procédures de tests techniques, de tests d'intégration et de recette de ces développements\", 'Participer à la conception et mise en place de datawarehouse', 'Encadrer des développeurs juniors', 'Prendre en charge la maintenance évolutive et corrective en conditions opérationnelles', \"Mettre en place et faire respecter les normes de développement et d'architecture définies\"], 'required_technical_skills': [\"flux d'alimentation\", 'traitement de données', 'Talend', 'Informatica', 'Stambia', 'Datastage Server', 'Oracle Data Integrator (ODI)', 'Tibco Jaspersoft', 'Azure Data Factory', 'Pentaho', 'SQL/PL-SQL', 'Java', 'bases de données on-premise et/ou cloud'], 'desirable_technical_skills': ['Talend Enterprise Data Intégration'], 'required_non_technical_skills': ['capacités relationnelles', 'rédactionnelles', 'analytiques', \"sens de l'écoute\", 'communication', \"capacité d'adaptation\", \"esprit d'équipe\"], 'tools_and_technologies': ['Talend', 'Informatica', 'Stambia', 'Datastage Server', 'Oracle Data Integrator (ODI)', 'Tibco Jaspersoft', 'Azure Data Factory', 'Pentaho', 'SQL', 'PL-SQL', 'Java']}}, {'id': '166JRCT', 'intitule': 'Senior Data Engineer (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': None}, {'id': '166HXCK', 'intitule': 'Data manager', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': ['Validation des spécifications générales et détaillées des produits et solutions', \"Animation réunion/échange avec les architectes de la donnée et le gestionnaire de l'offre marketing\", 'Conception technique détaillée des modèles pour la configuration', 'Implémentation de la solution technique et son intégration dans le flux E2E SAP', 'Élaboration des plans de tests unitaires et fonctionnels', 'Déploiement des produits et de leurs paramétrages dans les environnements de Ventes et Production', 'Suivi et maintenance des données de configurations', 'Support aux utilisateurs Front et Backoffice'], 'required_technical_skills': ['SAP ERP Administration des ventes (SD) / Production Planning (PP)', 'Variant Configurator (LO-VC)', 'Sales Solution Configurator (SSC)', \"Outils d'import/export et de transformation de données\", 'Netweaver Eclipse', 'SAP-SME (Solution Modeling Environment)', 'SAP NetWeaver 7.5 Stack Java /ECC 6.0', 'Amazon Web Services (AWS)', 'Cloud'], 'desirable_technical_skills': ['Python', 'Java', 'ABAP', 'langage de programmation sous contrainte'], 'required_non_technical_skills': ['Anglais niveau écrit/parlé courant (niveau B2 mini) dans un environnement technique/industriel', \"Bon sens de l'organisation\", 'Travail en équipe', 'Relationnel'], 'tools_and_technologies': ['SAP ERP Administration des ventes (SD)', 'Production Planning (PP)', 'Variant Configurator (LO-VC)', 'Sales Solution Configurator (SSC)', 'Netweaver Eclipse', 'SAP-SME (Solution Modeling Environment)', 'SAP NetWeaver 7.5', 'ECC 6.0', 'Amazon Web Services (AWS)', 'Cloud']}}, {'id': '166HTRW', 'intitule': 'Technicien backup Data Center (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': [\"Rackage, installation équipements, brassage, câblage, inventaire, mise à jour au niveau d'outils\", 'Accompagnement et montée en compétences', 'Adaptation aux dernières nouveautés et capacité à innover'], 'required_technical_skills': [], 'desirable_technical_skills': [], 'required_non_technical_skills': [], 'tools_and_technologies': []}}, {'id': '166HLYZ', 'intitule': 'DATA ENGINEER (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': [\"Concevoir et réaliser une solution Data de l'acquisition à l'exploitation de la donnée\", 'Collecter et intégrer les données nécessaires à la résolution de problématiques métier et opérationnelles', 'Garantir la qualité des données en appliquant les règles de Data Gouvernance et de Data Management', 'Transcrire les besoins métier en règles de gestion data', \"Industrialiser le déploiement des réalisations avec des tests unitaires, d'intégration et de non-régression\"], 'required_technical_skills': ['Conception de solution Data', 'Collecte et intégration de données', 'Data Gouvernance', 'Data Management'], 'desirable_technical_skills': [], 'required_non_technical_skills': [], 'tools_and_technologies': ['Business intelligence', 'Data Analyse', 'Développement orienté objet']}}, {'id': '166HFTP', 'intitule': 'Chef ou cheffe de projet Big Data (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': [\"Piloter un projet BIG DATA d'ingestion et de traitement de données\", \"Produire des reportings et analyses Adhoc sur l'analyse des sillons, les impacts des plages de travaux, la relation contractuelle avec le gestionnaire d'infrastructure\", 'Animer et produire les comptes-rendus des comités', \"Suivre l'avancement des travaux de l'équipe DSI\", 'Piloter les opérations de suivi de run et gérer les incidents de production', 'Produire le reporting à destination de la MOA et du responsable de domaine', 'Mettre en place et suivre les différents outils de pilotage', 'Tenir à jour les outils de suivi économique'], 'required_technical_skills': ['Big Data', 'Architecture', 'Sécurité', 'Performance'], 'desirable_technical_skills': ['Éco-conception'], 'required_non_technical_skills': ['Animation', 'Gestion de projet', 'Comptes-rendus', \"Suivi d'avancement\", 'Pilotage opérationnel', 'Reporting', \"Gestion d'incidents\", 'Pilotage économique'], 'tools_and_technologies': ['JIRA']}}, {'id': '166GTKV', 'intitule': 'Chargé de production performance Data Accès Fixe (H/F)', 'mots_cles': 'data', 'domaine': 'M18', 'processed_skills': {'missions': [\"Gérer et suivre les alertes sur le réseau d'accès fixe\", 'Vérifier en temps réel les informations et savoir corréler les alertes avec les évènements ou les incidents en cours', 'Relancer par mail et/ou par téléphone les différents interlocuteurs', \"Créer les tickets et assurer le pilotage technique jusqu'à la résolution\"], 'required_technical_skills': ['Support réseau télécom fixe', 'Informatique réseau', 'Routeurs IP', 'Protocole de routage', 'Connaissances des architectures réseaux fixes et mobiles', 'Connaissances IP', 'Profil Telecom', 'Expérience boucle local', 'Raccordement', 'Optique', 'FTTx', 'DSL'], 'desirable_technical_skills': [], 'required_non_technical_skills': ['Bonne communication orale et écrite', 'Capacité de formalisation', 'Capacité à accompagner et à former', 'Pédagogue et formateur', 'Rigueur, autonomie, respect des process', \"Esprit d'analyse, résolution de problème\", 'Bon relationnel'], 'tools_and_technologies': ['Routeurs IP', 'Protocole de routage', 'Connaissances des architectures réseaux fixes et mobiles', 'Connaissances IP', 'Raccordement', 'Optique', 'FTTx', 'DSL']}}]\n"
     ]
    }
   ],
   "source": [
    "processed_jobs=[]\n",
    "for job in df_jobs40_list:#job is a dict\n",
    "    jobdata={}\n",
    "    #print(job['id'])\n",
    "    #print(job['intitule'])\n",
    "    print(\"1\",jobdata)\n",
    "    jobdata['id']=job['id'] \n",
    "    print(\"2\",jobdata)\n",
    "    jobdata['intitule']=job['intitule']\n",
    "    print(\"3\",jobdata)\n",
    "    jobdata['mots_cles']= keyword\n",
    "    jobdata['domaine']= domaine\n",
    "    skills=process_job_description(job['description'])\n",
    "    jobdata['processed_skills']=skills\n",
    "    print(\"4\",jobdata)\n",
    "    print(\"\\n\")\n",
    "    processed_jobs.append(jobdata)\n",
    "print(processed_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7078dfa-3d0d-49f4-98c6-55aee869180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_jobs=[]\n",
    "for job in df_jobs40_list:\n",
    "    jobdata={}\n",
    "    job\n",
    "    skills=process_job_description(job['description'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "78a50171-df4a-495e-ae2f-c1858f5c08c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Pour une prise de poste tout début janvier 2024 en région parisienne, nous recherchons un(e) Referential (MDM) Data Manager SENIOR.\\n\\nEn renfort au sein de l'équipe Data Management, vous devrez être capable d'accompagner les collaborateurs grâce à une expertise sur la gestion de référentiel qui sera très opérationnelle.\\n\\nLes 2 missions principales :\\n-\\tPrendre en charge la gestion (création, validation, update) des listes de valeur (nomenclature internes et externes) sur notre périmètre de données prioritaire en lien avec les data architectes (en charge de la modélisation de la données) et la squad plateforme (en charge de l'implémentation)\\n-\\tPrendre en charge le cadrage et la spécification des premiers mécanismes de gestions de référentiels à mettre en place sur notre périmètre prioritaire (mapping entre référentiels globaux/locaux, mécanismes d'alimentation automatisés afin de garantir la consistance entre les référentiels, etc.) et accompagner la squad platforme pour l'implémentation des mécanismes qui auront été validés\\n\\nPROFIL RECHERCHE\\nVous êtes rigoureux, pragmatique et pédagogue, c'est ce qui vous permettra au mieux de réussir votre mission. Par ailleurs, vous êtes doté d'un esprit d'analyse et de synthèse, d'un bon relationnel et vous avez un bon esprit d'équipe.\\n\\nUn Bac +5 ainsi que 10 ans d'expérience sur ce poste sont impératifs. Une expérience en lien avec la gouvernance des données, idéalement dans le secteur bancaire ou assurance serait appréciée. Votre maîtrise de l'anglais tant à l'écrit qu'à l'oral est indispensable pour jouer pleinement votre rôle au sein du Groupe et vis-à-vis des filiales. Vous avez une bonne sensibilité aux problématiques métier et règlementaires, ainsi qu'à la gestion de projets. \\n\\nCOMPETENCES REQUISES\\n-\\t10 ans d'expérience minimum sur le poste IMPERATIF\\n-\\tMDM - Master Data Management\\n-\\tLangages SQL\\n-\\tBac +5\\n-\\tAnglais courant\\n\\nNous recherchons un profil SENIOR, c'est à dire avec plus de 10 ans d'expérience sur ce poste, disponible de suite, en région parisienne ou prêt à s'y rendre.\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jobs.loc[0]['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6169bc15-e618-4c94-be37-0b791f2960c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '166PTQP', 'intitule': 'Referential Data Manager (H/F)', 'processed_skills': {'missions': ['Gérer les listes de valeur sur notre périmètre de données prioritaires', 'Spécifier les mécanismes de gestion de référentiels à mettre en place sur notre périmètre prioritaire', \"Accompagner la squad plateforme pour l'implémentation des mécanismes validés\"], 'required_technical_skills': ['MDM', 'SQL'], 'desirable_technical_skills': [], 'required_non_technical_skills': ['Rigueur', 'Pragmatisme', 'Pédagogie', \"Esprit d'analyse et de synthèse\", 'Bon relationnel', \"Esprit d'équipe\", \"Maîtrise de l'anglais\", 'Sensibilité aux problématiques métier et règlementaires', 'Gestion de projets'], 'tools_and_technologies': []}}, {'id': '166LZBG', 'intitule': 'Data Engineer (H/F)', 'processed_skills': {'missions': ['Mise à disposition de données fiables', 'Exploitation des données', 'Application de la politique de gouvernance des données', 'Développement de la plateforme data', 'Diversification des sources de données', \"Fiabilité de l'alimentation des données\", \"Industrialisation du déploiement d'algorithmes de prédiction et de recommandation\", 'Construction de visions de données structurées', 'Maintenance et évolution des outils techniques (ETL, référentiels, dataviz)', 'Disponibilité et qualité des données', \"Sensibilisation et partage des connaissances avec l'équipe\"], 'required_technical_skills': [], 'desirable_technical_skills': [], 'required_non_technical_skills': [], 'tools_and_technologies': []}}, {'id': '166LKCF', 'intitule': 'DATA ENGINEER - BUSINESS INTELLIGENCE (H/F)', 'processed_skills': {'missions': ['Concevoir, développer et maintenir des pipelines de données et des processus ETL', 'Développer et optimiser des requêtes SQL, des scripts PySpark, des calculs DAX et des transformations Power Query M', 'Utiliser efficacement les services et outils Azure pour construire des solutions de données évolutives et performantes', \"Travailler en étroite collaboration avec l'équipe de gestion des données pour soutenir la gouvernance des données\", \"Superviser l'élaboration et la mise en œuvre d'un plan d'action pour l'amélioration de la qualité de l'eau et de l'assainissement\", \"Contribuer à la livraison et au succès d'un portefeuille de projets, notamment l'extraction de données, l'ingestion, la préparation de tables et la transformation de données, ainsi que les aspects de visualisation sur Power BI\", \"Documenter clairement et complètement les processus d'ingénierie des données\"], 'required_technical_skills': ['ETL', 'SQL', 'PySpark', 'DAX', 'Power Query M'], 'desirable_technical_skills': ['Azure', 'Azure Devops'], 'required_non_technical_skills': ['Gestion des données', 'Gouvernance des données'], 'tools_and_technologies': ['Shine', 'Azure Devops', 'Azure', 'Power BI']}}, {'id': '166LJVL', 'intitule': 'DATA ENGINEER - BUSINESS INTELLIGENCE (H/F)', 'processed_skills': {'missions': ['Concevoir, développer et maintenir des pipelines de données et des processus ETL', 'Développer et optimiser des requêtes SQL, scripts PySpark, calculs DAX et transformations Power Query M', 'Utiliser efficacement les services et outils Azure pour construire des solutions de données évolutives et performantes', \"Travailler en étroite collaboration avec l'équipe de gestion des données pour soutenir la gouvernance des données\", \"Superviser l'élaboration et la mise en œuvre d'un plan d'action pour l'amélioration de la qualité de l'eau et de l'assainissement\", \"Contribuer à la livraison et au succès d'un portefeuille de projets\"], 'required_technical_skills': ['ETL', 'SQL', 'PySpark', 'DAX', 'Power Query M', 'Azure', 'Power BI'], 'desirable_technical_skills': ['Shine', 'Azure Devops'], 'required_non_technical_skills': [\"Baccalauréat en informatique, en systèmes d'information ou dans un domaine connexe\", 'Expérience en tant que Data Engineer ou Data Analyst', \"Expérience dans le domaine de la finance ou de l'assurance\"], 'tools_and_technologies': ['ETL', 'SQL', 'PySpark', 'DAX', 'Power Query M', 'Azure', 'Power BI', 'Shine', 'Azure Devops']}}, {'id': '166KMWH', 'intitule': 'DATA ARCHITECT (H/F)', 'processed_skills': {'missions': ['Conseil, expertise et audits techniques', 'Cartographie des processus', \"Définition de Roadmaps et d'architectures Data (Cloud/On premise)\", \"Aide au choix d'outils\", 'PoC/MVP', 'Coaching/formations', 'Participation au recrutement et à la formation des jeunes talents', 'Veille technologique et innovation'], 'required_technical_skills': ['Architectures Data (Cloud/On premise)', 'ETL', 'Outil de dataviz', 'Modélisation de données'], 'desirable_technical_skills': [], 'required_non_technical_skills': [\"Travail d'équipe\", 'Autonomie', 'Curiosité technique', \"Sens de l'écoute et de la communication\", \"Capacité d'adaptation\"], 'tools_and_technologies': []}}, {'id': '166KMSV', 'intitule': 'RECHERCHE DATA MANAGER - (H/F)', 'processed_skills': {'missions': [\"Définition d'un schéma directeur des systèmes d'information\", \"Cadrage de l'aide au choix pour remplacer un ERP obsolète\", \"Pilotage d'un portefeuille projets\", 'Accompagnement dans le changement par la mise en place de formations adaptées', 'Mise en place de la gouvernance des données', \"Acculturation de la data et accompagnement dans le choix d'outils\", 'Facilitation du catalogage des données clés et guidage des activités', 'Mesure et gestion de la qualité des données avec des indicateurs de performance clés', \"Mise en œuvre des référentiels de données avec l'accent sur les architectures dédiées\"], 'required_technical_skills': ['BI', 'Data', 'Audit', \"Conseil en système d'information\", 'ERP', 'MDM', 'PIM', 'DAM', 'Gestion des solutions', 'Semarchy', 'Informatica', 'Blueway'], 'desirable_technical_skills': ['Gestion de projets', 'Management de service data'], 'required_non_technical_skills': ['Gestion et/ou direction de projets informatiques', \"Orientation vers l'accompagnement des entreprises\", \"Capacité de mettre en relation les enjeux de l'entreprise avec son secteur d'activité\", 'Présentation claire des enjeux de la data et des bonnes pratiques'], 'tools_and_technologies': [\"Systèmes d'information\", 'Formations', 'Gouvernance des données', 'Catalogage des données', 'Indicateurs de performance clés', 'Référentiels de données', 'Architectures dédiées']}}, {'id': '166KMJZ', 'intitule': 'Ingénieur Big Data H/F', 'processed_skills': {'missions': ['Intégration de solution big data', 'Développement des modèles de données', 'Développement des scripts de traitements', 'Automatisation des déploiements', 'Réalisation des maintenances évolutive / corrective'], 'required_technical_skills': ['Linux (Redhat)', 'Scripting : Python / PySpark', 'Automation (Ansible)', 'Data : Apache Airflow / Kafka / ElasticSearch / Spark / Nifi'], 'desirable_technical_skills': [], 'required_non_technical_skills': ['Organisation', 'Polyvalence', 'Rigueur', 'Curiosité'], 'tools_and_technologies': []}}, {'id': '166JWRH', 'intitule': 'Ingénieur Data Intégration Sénior (H/F)', 'processed_skills': {'missions': ['Analyser techniquement et fonctionnellement le besoin', 'Participer à la rédaction de spécifications techniques et aux procédures de livraison', 'Concevoir des procédures de transformation de données (maping)', 'Migrer et/ou développer des flux de données de type ETL/ELT/ESB', \"Développer des tests applicatifs, définir et réaliser les procédures de tests techniques, de tests d'intégration et de recette de ces développements\", 'Participer à la conception et mise en place de datawarehouse', 'Encadrer des développeurs juniors', 'Prendre en charge la maintenance évolutive et corrective en conditions opérationnelles', \"Mettre en place et faire respecter les normes de développement et d'architecture définies\"], 'required_technical_skills': [\"Développement de flux d'alimentation et de traitement de données\", \"Plateforme d'intégration de données: Talend, Informatica, Stambia, Datastage Server, Oracle Data Integrator (ODI), Tibco Jaspersoft, Azure Data Factory, Pentaho\", 'Langage SQL/PL-SQL', 'Java', 'Bases de données on-premise et/ou cloud'], 'desirable_technical_skills': ['Talend Enterprise Data Intégration'], 'required_non_technical_skills': ['Capacités relationnelles', 'Capacités rédactionnelles', 'Capacités analytiques', \"Sens de l'écoute\", 'Communication', 'Adaptabilité', \"Esprit d'équipe\"], 'tools_and_technologies': ['Talend', 'Informatica', 'Stambia', 'Datastage Server', 'Oracle Data Integrator (ODI)', 'Tibco Jaspersoft', 'Azure Data Factory', 'Pentaho', 'SQL', 'PL-SQL', 'Java', 'Bases de données on-premise', 'Bases de données cloud']}}, {'id': '166JRCT', 'intitule': 'Senior Data Engineer (H/F)', 'processed_skills': None}, {'id': '166HXCK', 'intitule': 'Data manager', 'processed_skills': {'missions': ['Validation des spécifications générales et détaillées des produits et solutions', \"Animation réunion/échange avec les architectes de la donnée et le gestionnaire de l'offre marketing\", 'Conception technique détaillée des modèles pour la configuration', 'Implémentation de la solution technique et son intégration dans le flux E2E SAP', 'Élaboration des plans de tests unitaires et fonctionnels', 'Déploiement des produits et de leurs paramétrages dans les environnements de Ventes et Production', 'Suivi et maintenance des données de configurations', 'Support aux utilisateurs Front et Backoffice'], 'required_technical_skills': ['SAP ERP Administration des ventes (SD) / Production Planning (PP)', 'Variant Configurator (LO-VC)', 'Sales Solution Configurator (SSC)', \"Outils d'import/export et de transformation de données (Web, Java, ABAP)\", 'Netweaver Eclipse', 'SAP-SME (Solution Modeling Environment)', 'SAP NetWeaver 7.5 Stack Java/ECC 6.0', 'Amazon Web Services (AWS)', 'Cloud'], 'desirable_technical_skills': ['Python', 'Java', 'ABAP', 'Langage de programmation sous contrainte'], 'required_non_technical_skills': ['Bonne organisation', 'Travail en équipe', 'Relationnel', \"Communication avec les différents métiers de l'entreprise\"], 'tools_and_technologies': ['SAP ERP', 'Variant Configurator (LO-VC)', 'Sales Solution Configurator (SSC)', 'Netweaver Eclipse', 'SAP-SME (Solution Modeling Environment)', 'SAP NetWeaver 7.5 Stack Java/ECC 6.0', 'Amazon Web Services (AWS)', 'Cloud']}}, {'id': '166HTRW', 'intitule': 'Technicien backup Data Center (H/F)', 'processed_skills': {'missions': [\"Rackage, installation équipements, brassage, câblage, inventaire, mise à jour au niveau d'outils.\", 'Accompagnement et formation pour monter en compétences', \"S'adapter aux dernières nouveautés et innover\"], 'required_technical_skills': [], 'desirable_technical_skills': [], 'required_non_technical_skills': ['Adaptabilité', \"Esprit d'innovation\"], 'tools_and_technologies': []}}, {'id': '166HLYZ', 'intitule': 'DATA ENGINEER (H/F)', 'processed_skills': {'missions': [\"Concevoir et réaliser une solution Data de l'acquisition à l'exploitation de la donnée\", 'Identifier, collecter et intégrer les données pour résoudre des problématiques métier et opérationnelles', 'Garantir la qualité des données et appliquer les règles de Data Gouvernance et de Data Management', 'Transcrire les besoins métier en règles de gestion data', \"Industrialiser le déploiement des réalisations avec des tests unitaires, d'intégration et de non-régression\"], 'required_technical_skills': ['Conception de solution data', 'Collecte et intégration de données', 'Data Gouvernance', 'Data Management', 'Tests unitaires', \"Tests d'intégration\", 'Tests de non-régression'], 'desirable_technical_skills': ['Business intelligence', 'Data Analyse', 'Développement orienté objet'], 'required_non_technical_skills': [], 'tools_and_technologies': ['Divers outils Data', 'Technologies de pointe']}}, {'id': '166HFTP', 'intitule': 'Chef ou cheffe de projet Big Data (H/F)', 'processed_skills': {'missions': [\"Piloter un projet BIG DATA d'ingestion et de traitement de données\", \"Produire des reportings et analyses adhoc sur les sillons ferroviaires, les plages travaux et la relation avec le gestionnaire d'infrastructure\", 'Animer et produire les comptes-rendus des réunions', \"Suivre l'avancement des travaux de l'équipe DSI\", 'Piloter les opérations de suivi de run et gérer les incidents de production', 'Produire le reporting à destination de la MOA et du responsable de domaine', 'Mettre en place et suivre les outils de pilotage', 'Tenir à jour les outils de suivi économique'], 'required_technical_skills': ['BIG DATA', \"systèmes d'information\", 'pilotage informatique', 'pilotage de projets'], 'desirable_technical_skills': ['architecture', 'sécurité', 'performance'], 'required_non_technical_skills': ['aptitude à appréhender les sujets techniques', \"appétence pour l'éco-conception\"], 'tools_and_technologies': ['JIRA']}}, {'id': '166GTKV', 'intitule': 'Chargé de production performance Data Accès Fixe (H/F)', 'processed_skills': {'missions': [\"Gérer et suivre les alertes sur le réseau d'accès fixe\", 'Vérifier en temps réel les informations et savoir corréler les alertes avec les évènements ou les incidents en cours', 'Relancer par mail et/ou par téléphone les différents interlocuteurs', \"Créer les tickets et assurer le pilotage technique jusqu'à la résolution\"], 'required_technical_skills': ['Support réseau télécom fixe', 'Informatique réseau', 'Routeurs IP', 'Protocole de routage'], 'desirable_technical_skills': ['Connaissances des architectures réseaux fixes et mobiles', 'Connaissances IP', 'Profil Telecom', 'Expérience boucle locale (raccordement, optique, FTTx et DSL)'], 'required_non_technical_skills': ['Bonne communication orale et écrite', 'Capacité de formalisation', 'Capacité à accompagner et à former', 'Pédagogue et formateur', 'Rigueur, autonomie, respect des process', \"Esprit d'analyse, résolution de problème\", 'Bon relationnel'], 'tools_and_technologies': ['Téléphone mobile', 'Routeurs IP']}}, {'id': '166GPDR', 'intitule': 'ADMINISTRATEUR BIG DATA/HADOOP (H/F)', 'processed_skills': {'missions': ['Designer et délivrer de nouvelles plateformes Big Data', 'Automatiser les tâches via des scripts Shell & Python, Terraform (IAC)', 'Réaliser le développement des playbooks avec Gitlab, Ansible et Jenkins', \"Assurer les scale up / down de cluster via l'ajout ou décommissionnement de nœuds\", 'Mettre en place des environnements de développements et de tests', \"Réaliser l'installation et la sécurisation des clusters\", 'Assurer les migrations, les performance tuning et le scaling', 'Résoudre les incidents de niveau N3', 'Travailler en approche agile (scrum) et devops'], 'required_technical_skills': ['Linux'], 'desirable_technical_skills': ['Python', 'Shell', 'SQL', 'Scala', 'VS Code', 'MongoDB', 'Git', 'Gitlab', 'Ansible', 'Jenkins', 'Linux Redhat', 'CentOS', 'Ubuntu'], 'required_non_technical_skills': ['excellent relationnel', 'rigueur', 'curiosité', 'orientation satisfaction client'], 'tools_and_technologies': ['Shell', 'Python', 'Terraform', 'Gitlab', 'Ansible', 'Jenkins', 'Linux', 'VS Code', 'MongoDB', 'Git', 'Linux Redhat', 'CentOS', 'Ubuntu']}}, {'id': '166GFWJ', 'intitule': 'Cheffe / Chef de projet Data Intégration (H/F)', 'processed_skills': {'missions': ['Gérer la relation avec les clients pour comprendre et analyser leurs besoins', 'Assurer la réalisation des projets en assurant un haut niveau de satisfaction client', 'Rédiger les documents et rapports associés à la conduite de ces missions', 'Être force de proposition et de réalisation dans le domaine EAI', 'Réaliser et maintenir des développements'], 'required_technical_skills': [], 'desirable_technical_skills': ['Data integration', 'EAI', 'ETL', 'DEX'], 'required_non_technical_skills': ['Gestion de projet', 'Relation client', 'Analyse des besoins', 'Rédaction', 'Proposition', 'Développement'], 'tools_and_technologies': ['DEX']}}, {'id': '166DSYV', 'intitule': 'Chef ou cheffe de projet expert data (H/F)', 'processed_skills': {'missions': ['Conception technico-fonctionnelle de NOTITIA', 'Valorisation des données', \"Accompagnement des métiers de l'immobilier sur la valorisation des données\", 'Conception, réalisation, qualification et maintenance de la plateforme', 'Conception, réalisation, qualification et maintenance des applications de data visualisation', 'Pilotage de la transformation des données et mise à disposition dans la plateforme', \"Industrialisation des flux d'alimentation de données\", \"Évolution de l'architecture technique de la plateforme DATA dans le cloud\", 'Documentation et garantie de la cohérence et qualité des données', 'Accompagnement des utilisateurs', 'Veille technologique sur les nouvelles technologies Big Data et IA'], 'required_technical_skills': ['Analyse de données', 'Gestion et valorisation de la Data', 'Statistiques', 'Maîtrise des technologies cloud (AWS, AZURE)', 'Workflow, flux de données', 'Smart data', 'Big Data', 'BI (Business Intelligence)', 'Data Analytics', 'SQL', 'Modélisation de données', 'Data visualization (Power BI)', \"Maîtrise d'un langage de programmation (Python, R)\", 'Extraction, transformation et chargement des données (ETL/ELT)'], 'desirable_technical_skills': [], 'required_non_technical_skills': [\"Diplôme BAC+5 minimum en école d'ingénieur ou à l'université\", \"5 ans minimum d'expérience dans la gestion et valorisation des données\"], 'tools_and_technologies': ['AWS', 'AZURE', 'SQL', 'Python', 'R', 'Power BI', 'ETL/ELT']}}, {'id': '166DBBK', 'intitule': 'Ingénieur Data Gouvernance - (H/F)', 'processed_skills': {'missions': [\"Participer à l'organisation des procédures de travail de l'équipe Data Office\", \"Veiller à l'application des règles et des procédures définies dans la Gouvernance des Données\", \"Identifier les données métiers et analyser les demandes d'accès à des données restreintes\", \"Analyser les risques liés à des données lors de l'ingestion sur une plateforme\", \"Faciliter l'application du RGPD dans les projets et dans les applications\", 'Maintenir le registre RGPD', \"S'assurer que les données sont valides et de qualité\", 'Maintenir le référentiel de Gouvernance des Données et le Data Catalog', 'Sensibiliser et former aux règles de la Gouvernance des Données', \"Participer à la construction des documentations de l'équipe Data Office\", 'Aider à la gestion des accès plateforme des clients'], 'required_technical_skills': ['Gouvernance des Données', 'Conduite de réunion', 'Conduite du changement'], 'desirable_technical_skills': [], 'required_non_technical_skills': ['Autonomie', 'Curiosité', 'Sens de la communication', 'Travail en équipe'], 'tools_and_technologies': ['RGPD']}}, {'id': '166CVQQ', 'intitule': 'Développeur / Développeuse Big Data', 'processed_skills': {'missions': ['Concevoir et développer des pipelines de données performants', 'Optimiser les performances des systèmes de traitement de données', \"Collaborer avec les équipes d'ingénierie pour intégrer les solutions Big Data aux applications existantes\", 'Participer à la veille technologique pour rester à la pointe des avancées dans le domaine'], 'required_technical_skills': ['Développement Big Data', 'Hadoop', 'Spark', 'Kafka', 'Java', 'Scala', 'Python', 'Manipulation de données', 'Architectures de données distribuées'], 'desirable_technical_skills': [], 'required_non_technical_skills': ['Résolution de problèmes complexes', \"Travail d'équipe\"], 'tools_and_technologies': ['Hadoop', 'Spark', 'Kafka', 'Java', 'Scala', 'Python']}}, {'id': '166BXHJ', 'intitule': 'Consultant Data Manager (H/F)', 'processed_skills': {'missions': ['Modéliser la gouvernance data', \"Diagnostic de maturité data et élaboration du portefeuille de cas d'usage\", \"Développer les procédures d'alimentation (ETL) et/ou de reporting\", 'Réaliser la recette et les tests', 'Pilotage de projets de transformation data', 'Exploration analytique des données, interprétation des résultats et élaboration de recommandations métiers', 'Mise en conformité réglementaire RGPD', 'Suivre et mettre en production', 'Rédiger de la documentation technique', 'Réaliser des formations auprès des utilisateurs'], 'required_technical_skills': [], 'desirable_technical_skills': [], 'required_non_technical_skills': ['Polyvalence'], 'tools_and_technologies': ['ETL']}}, {'id': '166BKSB', 'intitule': 'Data Engineer - H/F - CDI (H/F)', 'processed_skills': {'missions': [\"Développement, déploiement et maintenance de pipelines d'ETL/ELT\", \"Mise en place d'APIs et de backends applicatifs\", 'Définition et mise en place de modèles de données, administration de base de données', 'Utilisation de plateformes et outils Cloud', \"Compréhension et application de principes d'architecture\", \"Supervision et monitoring d'applications\"], 'required_technical_skills': ['Python', 'SQL', 'ETL', 'ELT', 'APIs', 'backends applicatifs', 'modèles de données', 'administration de base de données', 'plateformes et outils Cloud', \"principes d'architecture\", \"supervision d'applications\", \"monitoring d'applications\"], 'desirable_technical_skills': ['calcul distribué', 'traitement des données temps réel', 'bases de données NoSQL', 'AWS', 'GCP', 'Azure'], 'required_non_technical_skills': ['curiosité', 'challenge', \"esprit d'équipe\", \"volonté d'aider les autres\", 'maîtrise de la langue française'], 'tools_and_technologies': ['PostgreSQL', 'MongoDB', 'ELK', 'Airflow', 'Luigi', 'Git', 'CI/CD', 'Docker', 'Kubernetes', 'Nomad', 'Ansible', 'Snowflake', 'Databricks', 'Kafka']}}, {'id': '166BHFQ', 'intitule': 'Consultant Expert Data (H/F)', 'processed_skills': {'missions': ['Cadrer et/ou conduire des projets de référentiels', \"Définir et concevoir des échanges de données autour des projets de référentiels, des projets d'échanges inter-applicatifs\", \"Gouvernance des données : définition d'un cadre de gouvernance opérationnelle des données en coordination avec les métiers, pilotage de la gouvernance transverse\", 'Intervenir aux côtés des commerciaux lors de présentations ou lors de rendez-vous clients'], 'required_technical_skills': ['projets SI', 'entrepôt de données', 'datalake', 'EAI', 'ESB', 'MDM', 'API Management'], 'desirable_technical_skills': ['Anglais technique'], 'required_non_technical_skills': [\"capacité d'analyse\", 'communication', \"animation d'équipe\", \"prise d'initiative\", 'décision'], 'tools_and_technologies': []}}, {'id': '165ZZWS', 'intitule': 'Géomaticien / Data analyste (H/F)', 'processed_skills': {'missions': [\"Assurer la production, la gestion et l'organisation de données cartographiques\", 'Assurer la mise à disposition des données pour nos ingénieurs environnement', 'Produire ou maintenir des outils informatiques, par le biais de développements Front-End / Back-End, pour faciliter la mise à disposition de la donnée', 'Administrer la base de données ATDx', 'Gérer le parc informatique en lien avec notre prestataire de service'], 'required_technical_skills': ['SIG', 'ArcGIS', 'QGIS', 'Global Mapper', 'Base de données', 'MySQL', 'MariaDB', 'Langage SQL', 'Programmation', 'HTML', 'Javascript', 'PHP', 'Python'], 'desirable_technical_skills': ['AutoCAD', 'Covadis'], 'required_non_technical_skills': ['Organisé', 'Rigoureux', 'Force de proposition', 'Travail en équipe', 'Capacités relationnelles'], 'tools_and_technologies': ['SIG', 'ArcGIS', 'QGIS', 'Global Mapper', 'Base de données', 'MySQL', 'MariaDB', 'Langage SQL', 'Programmation', 'HTML', 'Javascript', 'PHP', 'Python', 'AutoCAD', 'Covadis']}}, {'id': '165YMYY', 'intitule': 'Responsable Data Patrimoine (H/F)', 'processed_skills': {'missions': [\"Participer à l'organisation et au déploiement du projet DATA\", \"Mener des actions de sensibilisation et d'accompagnement des services afin de fiabiliser les données\", 'Collecter et mettre à jour les données', 'Réaliser des contrôles pour fiabiliser la donnée', \"Piloter les campagnes de diagnostics immobiliers ainsi que les campagnes d'actualisation correspondantes\", 'Faire appliquer les process', 'Analyse des données / contrôle de gestion', \"Amiante : remontées des anomalies, mise en place d'actions correctives, sensibilisation et accompagnement des acteurs internes ou externes\", 'Sécurité du patrimoine : remontées des anomalies, suivi des actions correctives', \"Consommation énergétique : réalisation de tableaux de bord de suivi des consommations en vue d'une meilleure maîtrise des charges énergétiques des locataires\"], 'required_technical_skills': ['gestion de projet', 'DATA', 'techniques et métiers du bâtiment', 'hygiène', 'sécurité', \"gestion de l'énergie\", 'gestion de données en très grand nombre', 'rigueur', 'outil informatique'], 'desirable_technical_skills': ['connaissances dans les diagnostics immobiliers', 'contrôle qualité'], 'required_non_technical_skills': ['rigoureux', 'bon relationnel', \"esprit d'analyse\", 'esprit de synthèse'], 'tools_and_technologies': ['outils de gestion', 'outils de contrôle qualité']}}, {'id': '165YGFX', 'intitule': 'Data manager', 'processed_skills': {'missions': ['Fiabiliser les données tout au long de la mission', 'Assurer un contrôle de qualité permanent', 'Vérifier les cas particuliers sur le terrain', 'Intervenir à distance ou sur le terrain en cas de doute/incohérence', 'Faire le lien avec les équipes concernées pour le traitement des données remontées', 'Contrôler en continu la qualité des données saisies par les équipes terrain', 'Assurer la normalisation des données présentes dans la base', \"S'assurer du respect de la RGPD\"], 'required_technical_skills': ['Excel'], 'desirable_technical_skills': [], 'required_non_technical_skills': ['Sensibilité pour la data (base de données) / Analytique', 'Esprit logique', 'Polyvalence', 'Bonnes compétences en communication', 'Pédagogie'], 'tools_and_technologies': ['Pack Office']}}, {'id': '165XRTL', 'intitule': 'Data Engineer (H/F)', 'processed_skills': {'missions': ['Mettre en place et développer des solutions BIG Data et Cloud', \"Réaliser les tests unitaires et les tests d'intégration\", 'Analyser les logs applicatifs', 'Assurer le suivi de Production', 'Optimiser la chaîne de production'], 'required_technical_skills': ['GCP (Google Cloud Platform)', 'Hadoop', 'Spark', 'Scala', 'Kafka (Streaming)', 'SQL', 'NoSQL', 'Shell', 'Python'], 'desirable_technical_skills': [], 'required_non_technical_skills': ['Anglais'], 'tools_and_technologies': ['CI/CD', 'Agile Scrum']}}, {'id': '165XKVH', 'intitule': 'Data Manager transformation et gouvernance des filières (H/F)', 'processed_skills': {'missions': ['Accompagnement des Responsables de filières (10 salariés) et Responsables de données de chaque filière (~20 salariés)', 'Animation de la gouvernance des filières', 'Appui à la production et au suivi de la roadmap par filière', \"Définir les critères de segmentation des domaines et d'attribution des missions RDF/RDD\", 'Définir et suivre les KPI par filière', 'Concevoir et déployer une démarche « Data product »', \"Appui à l'usage de la plateforme Data pour les entités Matériel\", 'Identifier les besoins utiles aux expérimentations/projets PDA pour MD et MI', 'Intégrer les besoins à la roadmap des chantiers pour la PDA et suivre son avancement', 'Piloter et suivre les roadmaps de migration des stockages de données en local', 'Lancer et animer un réseau référents valorisation (environ 10 ressources)'], 'required_technical_skills': [], 'desirable_technical_skills': [], 'required_non_technical_skills': [\"Capacité d'animation\", 'Capacité de conduite de projets de transformation'], 'tools_and_technologies': ['Plateforme Data']}}, {'id': '165WQTC', 'intitule': 'Architecte Solutions Data (H/F)', 'processed_skills': {'missions': ['Définir et mettre en œuvre des architectures de données robustes', 'Conseiller sur les meilleures pratiques en architecture informatique', 'Participer à la planification et à la mise en œuvre de projets', 'Assurer une veille technologique constante', 'Encadrer et former les équipes'], 'required_technical_skills': ['gestion des données', 'bases de données', 'entrepôts de données', \"outils d'analyse\", 'intégration de bus Kafka'], 'desirable_technical_skills': ['provider Cloud'], 'required_non_technical_skills': [], 'tools_and_technologies': ['Kafka']}}, {'id': '165WCBB', 'intitule': 'Data Engineer Lyon H/F (H/F)', 'processed_skills': {'missions': [\"Participer à la conception et réalisation d'une solution Data depuis l'acquisition jusqu'à l'exploitation de la donnée en accompagnant la réflexion des directions métiers\", 'Identifier, collecter et intégrer les données nécessaires à la résolution de problématiques métier et opérationnelles', 'Garantir la qualité des données en mettant en place les outils de mesure et de suivi adéquats en appliquant les règles de Data Gouvernance et de Data Management', 'Transcrire des besoins métier en règles de gestion data', \"Industrialiser le déploiement de vos réalisations à travers l'implémentation de tests unitaires, d'intégration et de non-régression\"], 'required_technical_skills': ['SQL', 'Python', 'JAVA', 'Scala', 'framework de calcul distribué', 'intégration continue', 'Git', 'JUnit', 'SonarQube', 'Jenkins', 'cloud provider', 'AWS', 'GCP', 'Azure', 'distribution Big Data', 'Hortonworks', 'Cloudera', 'Datastax', 'modélisation relationnelle', 'modélisation non-relationnelle'], 'desirable_technical_skills': [], 'required_non_technical_skills': [\"diplôme d'ingénieur ou universitaire\", \"3 ans minimum d'expérience professionnelle dans le projet Data\", 'rigoureux', 'proactif', 'autonome', 'veille technologique', 'force de proposition', 'capacité à prendre de la hauteur', 'adaptabilité'], 'tools_and_technologies': ['AWS', 'GCP', 'Azure', 'Hortonworks', 'Cloudera', 'Datastax', 'SQL', 'Python', 'JAVA', 'Scala', 'Git', 'JUnit', 'SonarQube', 'Jenkins']}}, {'id': '165WBWS', 'intitule': 'Data Engineer Ile-de-France H/F (H/F)', 'processed_skills': {'missions': ['Concevoir et réaliser une solution Data', 'Identifier, collecter et intégrer les données nécessaires', 'Garantir la qualité des données', 'Transcrire des besoins métier en règles de gestion data', 'Industrialiser le déploiement des réalisations'], 'required_technical_skills': ['SQL', 'Python', 'JAVA', 'Scala', 'framework de calcul distribué', 'Git', 'JUnit', 'SonarQube', 'Jenkins', 'cloud provider (AWS, GCP, Azure)', 'distribution Big Data (Hortonworks, Cloudera, Datastax)', 'modélisation relationnelle', 'modélisation non-relationnelle'], 'desirable_technical_skills': [], 'required_non_technical_skills': [\"diplôme d'ingénieur ou universitaire\", 'expérience professionnelle de 3 ans minimum en projet Data', 'rigueur', 'proactivité', 'autonomie', 'veille technologique', 'force de proposition', 'capacité à prendre de la hauteur', 'adaptabilité'], 'tools_and_technologies': ['AWS', 'GCP', 'Azure', 'Hortonworks', 'Cloudera', 'Datastax']}}, {'id': '165TWBY', 'intitule': 'Consultant Data Governance (H/F)', 'processed_skills': {'missions': ['Accompagner les clients dans leur transformation data-driven', 'Transformer les clients sur leur stratégie data, leurs usages, leur culture data', 'Mettre en place la gouvernance des données', 'Définir le data ownership model et les processus de décision associés', \"Aider au choix d'outils de gouvernance / data catalog et accompagner leur mise en œuvre\", \"Accompagner la création et l'animation de communautés data et définir/mener des actions de change\", \"Contribuer aux sujets internes du cabinet, par exemple à l'élaboration de nos offres de valeur\", 'Intervenir sur les avant-vente'], 'required_technical_skills': ['Data governance', \"Systèmes d'information\", 'Organisation', 'Change management', 'Process engineering'], 'desirable_technical_skills': ['Collibra', 'DataGalaxy', 'Informatica'], 'required_non_technical_skills': ['Conseil', 'Curiosité', 'Proactivité', 'Feedback', 'Esprit de collaboration'], 'tools_and_technologies': ['Notion', 'Miro', 'Collibra', 'DataGalaxy', 'Informatica', 'Zeenea']}}, {'id': '165TVYK', 'intitule': 'Consultant Data Catalog (H/F)', 'processed_skills': {'missions': [\"Aider au choix d'outils de gouvernance / data catalog\", \"Paramétrer le data catalog selon le contexte client et les capacités technico-fonctionnelles de l'outil\", 'Accompagner la mise en œuvre du data catalog (modélisation conceptuelle des objets métiers, alimentation, connecteurs, etc.)', \"Favoriser l'adoption du data catalog auprès des équipes métier et IT, par la pédagogie / valorisation de ses usages et ainsi faire évoluer la culture data\", 'Contribuer au déploiement de la gouvernance des données (data ownership model, animation de la communauté data, comités data)'], 'required_technical_skills': ['data governance', \"systèmes d'information\", 'organisation', 'change management', 'process engineering', 'data catalog'], 'desirable_technical_skills': ['Collibra', 'DataGalaxy', 'Informatica'], 'required_non_technical_skills': ['curiosité', 'proactivité', 'esprit de collaboration', 'communication', 'management'], 'tools_and_technologies': ['Notion', 'Miro', 'Collibra', 'DataGalaxy', 'Informatica', 'Zeenea']}}, {'id': '165TRYD', 'intitule': 'Chef de projet informatique / Data manager (H/F)', 'processed_skills': {'missions': ['Conduite de projets liés aux applicatifs métiers', \"Animation d'un groupe de référents métiers (RMO)\", 'Veiller à la qualité de la donnée enregistrée dans les bases de données'], 'required_technical_skills': ['Gestion de projets', 'Rédaction de cahiers des charges', 'Mise en œuvre technique des solutions', 'Tests et déploiement', 'Formation des utilisateurs aux outils', 'Connaissance des environnements techniques (infrastructure système & réseau, bases de données)', 'Langage SQL'], 'desirable_technical_skills': ['Recensement des besoins', 'Évolution des procédures métiers', 'Mises à jour des applications métiers', 'Gestion de projets techniques', \"Optimisation de l'utilisation des outils\"], 'required_non_technical_skills': [\"Capacité d'animation\", 'Capacité de dialogue', \"Capacité d'écoute\", \"Capacité d'analyse\", 'Capacité de synthèse', 'Autonomie', \"Sens de l'organisation\", 'Travail en équipe', 'Bonne expression orale', 'Bonne expression écrite', 'Force de proposition'], 'tools_and_technologies': ['Outils actuels', 'Outils existants', 'Technicien infrastructure système & réseau', 'Technicien applicatifs métiers & mobilité', 'Service de Contrôle Interne']}}, {'id': '165SVZV', 'intitule': 'CHEF DE PROJET DATA SCIENCE (H/F)', 'processed_skills': {'missions': [\"Accompagnement et aide à la mise en oeuvre de la migration de l'ensemble des projets Data Science actuels vers la nouvelle plateforme data science\", \"Suivi de tous les projets Datalake pour la Loterie, de son expression de besoin détaillée jusqu'à sa mise en oeuvre, via la méthode SAFE\", \"Suivi et accompagnement d'une squad dédiée à la mise en place d'un moteur de recommandation d'offres sur le datalake\"], 'required_technical_skills': [], 'desirable_technical_skills': [], 'required_non_technical_skills': [], 'tools_and_technologies': []}}, {'id': '165SGDD', 'intitule': 'AUDACIEUX-SE CONSULTANT-E BI / BIG DATA - MONTPELLIER - (H/F)', 'processed_skills': {'missions': ['Travailler sur des projets décisionnels et Big Data', 'Intervenir en Conseil, Réalisation, Expertise, Formation et Développement', 'Avoir des missions variées auprès de Grands Groupes, PME & TPE', 'Gérer des projets de A à Z'], 'required_technical_skills': ['Informatique décisionnelle'], 'desirable_technical_skills': ['ETL / ELT : Talend DI', 'ETL / ELT : Semarchy xDI', 'ETL / ELT : MyReport Data', 'Dataviz : Power BI', 'Dataviz : Qlik Sense', 'Dataviz : MyReport Builder', 'Formation et certifications : IBM, Oracle, Microsoft'], 'required_non_technical_skills': ['Collaboration', 'Gestion de projets'], 'tools_and_technologies': ['ETL / ELT: Talend DI', 'ETL / ELT: Semarchy xDI', 'ETL / ELT: MyReport Data', 'Dataviz: Power BI', 'Dataviz: Qlik Sense', 'Dataviz: MyReport Builder', 'IBM', 'Oracle', 'Microsoft', 'Grands Groupes', 'PME', 'TPE']}}, {'id': '165SFZW', 'intitule': 'DATA ENGINEER SCALA/SPARK JUNIOR - H/F (H/F)', 'processed_skills': {'missions': ['Participer au développement des projets Data Engineering', \"Contribuer proactivement à la qualité et aux compétences de l'équipe Data Science et Engineering : veille techno, capitalisation, retours d'expérience, revues de code, et formations\"], 'required_technical_skills': ['Python', 'Java/Scala', 'Spark', 'Pandas', 'SQL', 'CI/CD'], 'desirable_technical_skills': ['Azure', 'AWS', 'Fullstack development'], 'required_non_technical_skills': ['Communication écrite et orale', 'Capacité de synthèse', \"Curiosité, autonomie et prise d'initiative\", 'Software craftmanship'], 'tools_and_technologies': ['Full cloud (AWS et Azure)', 'Python & Scala', 'Spark', 'Fullstack, API HTTP', 'Serverless', 'Bases relationnelles et NoSQL', 'CI (Gitlab, Jenkins, Nexus, SonarQube) et CD (Ansible)', 'Conteneurisation']}}, {'id': '165SFDN', 'intitule': 'Cheffe / Chef de projet Data Intégration (H/F)', 'processed_skills': {'missions': ['Gérer la relation avec les clients pour comprendre et analyser leurs besoins', 'Assurer la réalisation des projets en assurant un haut niveau de satisfaction client', 'Rédiger les documents et rapports associés à la conduite de ces missions', 'Être force de proposition et de réalisation dans le domaine EAI', 'Réaliser et maintenir des développements'], 'required_technical_skills': ['Dat intégration', 'Chef de projet informatique'], 'desirable_technical_skills': ['ETL DataExchanger'], 'required_non_technical_skills': ['Gestion de projet', 'Relation client', 'Analyse de besoins', 'Rédaction de documents', 'Force de proposition'], 'tools_and_technologies': ['ETL', 'EAI', 'DEX']}}, {'id': '165SFCR', 'intitule': 'Cheffe / Chef de projet Data Intégration (H/F)', 'processed_skills': {'missions': ['Gérer la relation avec les clients pour comprendre et analyser leurs besoins', 'Assurer la réalisation des projets en assurant un haut niveau de satisfaction client', 'Rédiger les documents et rapports associés à la conduite de ces missions', 'Être force de proposition et de réalisation dans le domaine EAI', 'Réaliser et maintenir des développements'], 'required_technical_skills': ['Data Integration', 'ETL', 'DEX'], 'desirable_technical_skills': [], 'required_non_technical_skills': ['Relation client', 'Analyse des besoins', 'Rédaction', 'Force de proposition', 'Développement'], 'tools_and_technologies': ['EAI']}}, {'id': '165RBLJ', 'intitule': 'Développeur Fullstack en IA et Data (H/F)', 'processed_skills': None}, {'id': '165QXBS', 'intitule': 'Développeur / Développeuse - Data transfert (H/F)', 'processed_skills': {'missions': ['Assurer la programmation des modules logiciels', 'Assurer les tests unitaires', 'Assurer la gestion des versions', 'Assurer les reprises de données en lien avec nos clients', 'Assurer le suivi de ces transferts de données', 'Maintenir et faire évoluer nos outils de transfert de données', 'Veiller en particulier aux respects des règles de bonnes pratiques en adoptant les normes de programmation en vigueur', 'Veiller aux respects des règles de sécurité'], 'required_technical_skills': ['base de données relationnelle', 'SQL'], 'desirable_technical_skills': [], 'required_non_technical_skills': [], 'tools_and_technologies': []}}, {'id': '165PWPX', 'intitule': 'TALENTUEUX CONSULTANT BI / BIG DATA - LILLE - (H/F)', 'processed_skills': {'missions': ['Apporter son expertise sur des projets décisionnels et Big Data', 'Intervenir en Conseil, Réalisation, Expertise, Formation et Développement', 'Travailler sur des technologies innovantes et des secteurs divers'], 'required_technical_skills': ['Informatique décisionnelle'], 'desirable_technical_skills': ['ETL/ELT', 'Talend DI', 'Semarchy xDI', 'MyReport Data', 'Dataviz', 'Power BI', 'Qlik Sense', 'MyReport Builder'], 'required_non_technical_skills': [], 'tools_and_technologies': ['BI', 'Big Data']}}]\n"
     ]
    }
   ],
   "source": [
    "print(processed_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa544e0e-2e1c-40c4-b75f-829db3bb49fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
